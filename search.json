[{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Started with Literature-Based Discovery","text":"Literature-based discovery (LBD) powerful approach identifying hidden connections existing knowledge scientific literature. vignette introduces LBDiscover package, provides tools automated literature-based discovery analyzing biomedical publications.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"installation","dir":"Articles","previous_headings":"Introduction","what":"Installation","title":"Getting Started with Literature-Based Discovery","text":"can install package CRAN: can install development version LBDiscover GitHub:","code":"install.packages(\"LBDiscover\") # install.packages(\"devtools\") devtools::install_github(\"chaoliu-cl/LBDiscover\")"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"basic-workflow","dir":"Articles","previous_headings":"Introduction","what":"Basic Workflow","title":"Getting Started with Literature-Based Discovery","text":"typical workflow literature-based discovery package consists : Retrieving publications PubMed sources Preprocessing text data Extracting biomedical entities Creating co-occurrence matrix Applying discovery models Visualizing evaluating results Let’s walk comprehensive example exploring connections migraine research.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"example-exploring-migraine-research","dir":"Articles","previous_headings":"Introduction","what":"Example: Exploring Migraine Research","title":"Getting Started with Literature-Based Discovery","text":"example, ’ll explore potential discoveries migraine research applying improved ABC model approach various utility functions.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"load-the-package","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"1. Load the package","title":"Getting Started with Literature-Based Discovery","text":"","code":"library(LBDiscover) #> Loading LBDiscover package"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"define-the-primary-term-of-interest","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"2. Define the primary term of interest","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Define the primary term of interest for our analysis primary_term <- \"migraine\""},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"retrieve-publications","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"3. Retrieve publications","title":"Getting Started with Literature-Based Discovery","text":"’ll search articles migraine pathophysiology treatment.","code":"# Search for migraine-related articles migraine_articles <- pubmed_search(   query = paste0(primary_term, \" pathophysiology\"),   max_results = 1000 )  # Search for treatment-related articles drug_articles <- pubmed_search(   query = \"neurological drugs pain treatment OR migraine therapy OR headache medication\",   max_results = 1000 )  # Combine and remove duplicates all_articles <- merge_results(migraine_articles, drug_articles) cat(\"Retrieved\", nrow(all_articles), \"unique articles\\n\") #> Retrieved 1925 unique articles"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"extract-variations-of-the-primary-term","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"4. Extract variations of the primary term","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Extract variations of our primary term using the utility function primary_term_variations <- get_term_vars(all_articles, primary_term) cat(\"Found\", length(primary_term_variations), \"variations of\", primary_term, \"in the corpus:\\n\") #> Found 13 variations of migraine in the corpus: print(head(primary_term_variations, 10)) #>  [1] \"migraine\"    \"Migraine\"    \"MigrainE\"    \"migraines\"   \"Migraines\"   #>  [6] \"migraineur\"  \"ofmigraine\"  \"Migraineux\"  \"migraineurs\" \"Migraineurs\""},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"preprocess-text-data","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"5. Preprocess text data","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Preprocess text preprocessed_articles <- preprocess_text(   all_articles,   text_column = \"abstract\",   remove_stopwords = TRUE,   min_word_length = 2  # Set min_word_length to capture short terms ) #> Tokenizing text..."},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"create-a-custom-dictionary","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"6. Create a custom dictionary","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Create a custom dictionary with all variations of our primary term custom_dictionary <- data.frame(   term = c(primary_term, primary_term_variations),   type = rep(\"disease\", length(primary_term_variations) + 1),   id = paste0(\"CUSTOM_\", 1:(length(primary_term_variations) + 1)),   source = rep(\"custom\", length(primary_term_variations) + 1),   stringsAsFactors = FALSE )  # Define additional MeSH queries for extended dictionaries mesh_queries <- list(   \"disease\" = paste0(primary_term, \" disorders[MeSH] OR headache disorders[MeSH]\"),   \"protein\" = \"receptors[MeSH] OR ion channels[MeSH]\",   \"chemical\" = \"neurotransmitters[MeSH] OR vasoactive agents[MeSH]\",   \"pathway\" = \"signal transduction[MeSH] OR pain[MeSH]\",   \"drug\" = \"analgesics[MeSH] OR serotonin agonists[MeSH] OR anticonvulsants[MeSH]\",   \"gene\" = \"genes[MeSH] OR channelopathy[MeSH]\" )  # Sanitize the custom dictionary custom_dictionary <- sanitize_dictionary(   custom_dictionary,   term_column = \"term\",   type_column = \"type\",   validate_types = FALSE  # Don't validate custom terms as they're trusted ) #> Sanitizing dictionary with 14 terms... #> Sanitization complete. 14 terms remaining (100% of original)"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"extract-biomedical-entities","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"7. Extract biomedical entities","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Extract entities using our custom dictionary custom_entities <- extract_entities(   preprocessed_articles,   text_column = \"abstract\",   dictionary = custom_dictionary,   case_sensitive = FALSE,   overlap_strategy = \"priority\",   sanitize_dict = FALSE  # Already sanitized )  # Extract entities using the standard workflow with improved entity validation # Check if running in R CMD check environment is_check <- !interactive() &&              (!is.null(Sys.getenv(\"R_CHECK_RUNNING\")) &&               Sys.getenv(\"R_CHECK_RUNNING\") == \"true\")               # More robust check for testing environment if (!is_check && !is.null(Sys.getenv(\"_R_CHECK_LIMIT_CORES_\"))) {   is_check <- TRUE }  # Set number of cores based on environment num_cores_to_use <- if(is_check) 1 else 4  standard_entities <- extract_entities_workflow(   preprocessed_articles,   text_column = \"abstract\",   entity_types = c(\"disease\", \"drug\", \"gene\"),   parallel = !is_check,           # Disable parallel in check environment   num_cores = num_cores_to_use,   # Use 1 core in check environment   batch_size = 500                # Process 500 documents per batch ) #> Warning in extract_entities_workflow(preprocessed_articles, text_column = #> \"abstract\", : UMLS source requested but no API key provided. Skipping UMLS.  # Uncomment to include UMLS entities # standard_entities <- extract_entities_workflow( #   preprocessed_articles, #   text_column = \"abstract\", #   entity_types = c(\"disease\", \"drug\", \"gene\", \"protein\", \"pathway\", \"chemical\"), #   dictionary_sources = c(\"local\", \"mesh\", \"umls\"),  # Including UMLS #   additional_mesh_queries = mesh_queries, #   sanitize = TRUE, #   api_key = \"your-umls-api-key\",  # Your UMLS API key here #   parallel = TRUE, #   num_cores = 4 # )  # Combine entity datasets using our utility function entities <- merge_entities(   custom_entities,   standard_entities,   primary_term ) #> Combined 6383 custom entities with 8929 standard entities.  # Filter entities to ensure only relevant biomedical terms are included filtered_entities <- valid_entities(   entities,   primary_term,   primary_term_variations,   validation_function = is_valid_biomedical_entity ) #> Filtered from 9338 to 9338 validated entities  # View the first few extracted entities head(filtered_entities) #>         entity entity_type doc_id start_pos end_pos #> 1 antimigraine     disease    251       368     379 #> 2 antimigraine     disease    251       667     678 #> 3 antimigraine     disease    292      1188    1199 #> 4 antimigraine     disease    292       876     887 #> 5 antimigraine     disease    292       632     643 #> 6 antimigraine     disease    388       670     681 #>                                                                                                                                                                                                                                                                                                                      sentence #> 1                                                                 However, the administration of antimigraine drugs in conventional oral pharmaceutical dosage forms is a challenge, since many molecules have difficulty crossing the blood-brain barrier (BBB) to reach the brain, which leads to bioavailability problems. #> 2                                                                                                                                                                                                                           Efforts have been made to find alternative delivery systems and/or routes for antimigraine drugs. #> 3                   The existence of patients with medication-resistant migraine may be due to the: (i) complex migraine pathophysiology, in which several systems appear to be deregulated before, during, and after a migraine attack; and (ii) pharmacodynamic and pharmacokinetic properties of antimigraine medications. #> 4                                                                                                                                                          EXPERT OPINION: Current anti-CGRPergic medications, although effective, have limitations, such as side effects and lack of antimigraine efficacy in some patients. #> 5 AREAS COVERED: By searching multiple electronic scientific databases, this narrative review examined: (i) the role of CGRP in migraine; and (ii) the current knowledge on the effects of CGRPergic antimigraine pharmacotherapies, including a brief analysis of their pharmacodynamic and pharmacokinetic characteristics. #> 6                                                                                                                                    Behavioral analysis, antioxidant assay, immunohistochemistry (IHC), histopathological examination, ELISA, and RT-PCR were conducted to evaluate the antimigraine potential of genistein. #>   frequency #> 1        13 #> 2        13 #> 3        13 #> 4        13 #> 5        13 #> 6        13"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"create-co-occurrence-matrix","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"8. Create co-occurrence matrix","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Create co-occurrence matrix with validated entities co_matrix <- create_comat(   filtered_entities,   doc_id_col = \"doc_id\",   entity_col = \"entity\",   type_col = \"entity_type\",   normalize = TRUE,   normalization_method = \"cosine\" ) #> Building entity-document matrix... #> Calculating co-occurrence matrix... #> Normalizing co-occurrence matrix using cosine method...  # Find our primary term in the co-occurrence matrix a_term <- find_term(co_matrix, primary_term) #> Found primary term in co-occurrence matrix  # Check matrix dimensions dim(co_matrix) #> [1] 19 19"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"apply-the-improved-abc-model","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"9. Apply the improved ABC model","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Apply the improved ABC model with enhanced term filtering and type validation abc_results <- abc_model(   co_matrix,   a_term = a_term,   c_term = NULL,  # Allow all potential C terms   min_score = 0.001,  # Lower threshold to capture more potential connections   n_results = 500,    # Increase to get more candidates before filtering   scoring_method = \"combined\",   # Focus on biomedically relevant entity types   b_term_types = c(\"protein\", \"gene\", \"pathway\", \"chemical\"),   c_term_types = c(\"drug\", \"chemical\", \"protein\", \"gene\"),   exclude_general_terms = TRUE,  # Enable enhanced term filtering   filter_similar_terms = TRUE,   # Remove terms too similar to migraine   similarity_threshold = 0.7,    # Relatively strict similarity threshold   enforce_strict_typing = TRUE   # Enable strict entity type validation ) #> Filtered 8 B terms (61.5%) that weren't valid biomedical entities #> Filtered 5 B terms that didn't match specified entity types: protein, gene, pathway, chemical #> No suitable B terms found with association score > 0.001 after filtering  # If we don't have enough results, try with less stringent criteria min_desired_results <- 10 if (nrow(abc_results) < min_desired_results) {   cat(\"Not enough results with strict filtering. Trying with less stringent criteria...\\n\")   abc_results <- abc_model(     co_matrix,     a_term = a_term,     c_term = NULL,     min_score = 0.0005,  # Even lower threshold     n_results = 500,     scoring_method = \"combined\",     b_term_types = NULL,  # No type constraints     c_term_types = NULL,  # No type constraints     exclude_general_terms = TRUE,     filter_similar_terms = TRUE,     similarity_threshold = 0.8,    # More lenient similarity threshold     enforce_strict_typing = FALSE  # Disable strict type validation as fallback   ) } #> Not enough results with strict filtering. Trying with less stringent criteria... #> Filtered 8 B terms (61.5%) that weren't valid biomedical entities #> Filtered out 0 B terms that were too similar to A term (similarity threshold: 0.8) #> Filtered 8 potential C terms that weren't valid biomedical entities #> Identifying potential C terms via 5 B terms... #>   |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |============================                                          |  40%  |                                                                              |==========================================                            |  60%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%  # View top results head(abc_results[, c(\"a_term\", \"b_term\", \"c_term\", \"abc_score\", \"b_type\", \"c_type\")]) #>               a_term     b_term       c_term  abc_score  b_type  c_type #> thrombosis1 migraine thrombosis      heparin 0.05907662 disease    drug #> headache2   migraine   headache      heparin 0.05746834 symptom    drug #> headache1   migraine   headache   enoxaparin 0.05632963 symptom    drug #> headache3   migraine   headache  myocarditis 0.05515907 symptom disease #> headache    migraine   headache azithromycin 0.05207699 symptom    drug #> headache4   migraine   headache  tocilizumab 0.05207699 symptom    drug"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"apply-statistical-validation-to-the-results","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"10. Apply statistical validation to the results","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Apply statistical validation to the results validated_results <- tryCatch({   validate_abc(     abc_results,     co_matrix,     alpha = 0.1,  # More lenient significance threshold     correction = \"BH\",  # Benjamini-Hochberg correction for multiple testing     filter_by_significance = FALSE  # Keep all results but mark significant ones   ) }, error = function(e) {   cat(\"Error in statistical validation:\", e$message, \"\\n\")   cat(\"Using original results without validation...\\n\")   # Add dummy p-values based on ABC scores   abc_results$p_value <- 1 - abc_results$abc_score / max(abc_results$abc_score, na.rm = TRUE)   abc_results$significant <- abc_results$p_value < 0.1   return(abc_results) }) #> Using optimized approach for large matrix validation... #> Using metadata for document count: 1692 #> Calculating statistical significance using hypergeometric test... #> 0.0% of connections are statistically significant (p < 0.10, BH correction)  # Sort by ABC score and take top results validated_results <- validated_results[order(-validated_results$abc_score), ] top_n <- min(100, nrow(validated_results))  # Larger top N for diversification top_results <- head(validated_results, top_n)  # View top validated results head(top_results[, c(\"a_term\", \"b_term\", \"c_term\", \"abc_score\", \"p_value\", \"significant\")]) #>               a_term     b_term       c_term  abc_score p_value significant #> thrombosis1 migraine thrombosis      heparin 0.05907662       1       FALSE #> headache2   migraine   headache      heparin 0.05746834       1       FALSE #> headache1   migraine   headache   enoxaparin 0.05632963       1       FALSE #> headache3   migraine   headache  myocarditis 0.05515907       1       FALSE #> headache    migraine   headache azithromycin 0.05207699       1       FALSE #> headache4   migraine   headache  tocilizumab 0.05207699       1       FALSE"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"diversify-and-ensure-minimum-results","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"11. Diversify and ensure minimum results","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Diversify results using our utility function diverse_results <- safe_diversify(   top_results,   diversity_method = \"both\",   max_per_group = 5,   min_score = 0.0001,   min_results = 5 )  # Ensure we have enough results for visualization diverse_results <- min_results(   diverse_results,   top_results,   a_term,   min_results = 3 )"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"visualize-the-results","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"12. Visualize the results","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Create heatmap visualization plot_heatmap(   diverse_results,   output_file = \"migraine_heatmap.png\",   width = 1200,   height = 900,   top_n = 15,   min_score = 0.0001,   color_palette = \"blues\",   show_entity_types = TRUE ) #> No connections are statistically significant (p < 0.05) #> Created heatmap visualization: migraine_heatmap.png  # Create network visualization plot_network(   diverse_results,   output_file = \"migraine_network.png\",   width = 1200,   height = 900,   top_n = 15,   min_score = 0.0001,   node_size_factor = 5,   color_by = \"type\",   title = \"Migraine Treatment Network\",   show_entity_types = TRUE,   label_size = 1.0 ) #> Created network visualization: migraine_network.png"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"create-interactive-visualizations","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"13. Create interactive visualizations","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Create interactive HTML network visualization export_network(   diverse_results,   output_file = \"migraine_network.html\",   top_n = min(30, nrow(diverse_results)),   min_score = 0.0001,   open = FALSE  # Don't automatically open in browser )  # Create interactive chord diagram export_chord(   diverse_results,   output_file = \"migraine_chord.html\",   top_n = min(30, nrow(diverse_results)),   min_score = 0.0001,   open = FALSE ) #> Number of unique terms: 9 #> First few terms: migraine, thrombosis, headache, fatigue, heparin #> Role assignments: A=1, B=3, C=5"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"evaluate-literature-support-and-generate-report","dir":"Articles","previous_headings":"Introduction > Example: Exploring Migraine Research","what":"14. Evaluate literature support and generate report","title":"Getting Started with Literature-Based Discovery","text":"","code":"# Evaluate literature support for top connections evaluation <- eval_evidence(   diverse_results,   max_results = 5,   base_term = \"migraine\",   max_articles = 5 ) #>  #> === Evaluation of Top Results === #>  #> Evaluating potential treatment: heparin  (drug)  #> ABC score: 0.0591  #> P-value: 1 - Not statistically significant  #> Connection through intermediary: thrombosis #> Found 4 articles directly linking migraine and heparin  #> Most recent article: Stroke Following Blunt Head Trauma: A Case Report and Review of the Literature.  #>  #> Evaluating potential treatment: heparin  (drug)  #> ABC score: 0.0575  #> P-value: 1 - Not statistically significant  #> Connection through intermediary: headache #> Found 4 articles directly linking migraine and heparin  #> Most recent article: Stroke Following Blunt Head Trauma: A Case Report and Review of the Literature.  #>  #> Evaluating potential treatment: enoxaparin  (drug)  #> ABC score: 0.0563  #> P-value: 1 - Not statistically significant  #> Connection through intermediary: headache #> Found 5 articles directly linking migraine and enoxaparin  #> Most recent article: Cerebral Venous Sinus Thrombosis Following Varicella Infection: A Case Report.  #>  #> Evaluating potential treatment: myocarditis  (disease)  #> ABC score: 0.0552  #> P-value: 1 - Not statistically significant  #> Connection through intermediary: headache #> Found 4 articles directly linking migraine and myocarditis  #> Most recent article: Brain white matter hyperintensities in Kawasaki disease: A case-control study.  #>  #> Evaluating potential treatment: azithromycin  (drug)  #> ABC score: 0.0521  #> P-value: 1 - Not statistically significant  #> Connection through intermediary: headache #> Found 1 articles directly linking migraine and azithromycin  #> Most recent article: A comparison of ciprofloxacin, norfloxacin, ofloxacin, azithromycin and cefixime examined by observational cohort studies.  # Prepare articles for report generation articles_with_years <- prep_articles(all_articles) #> Found 1925 articles with valid publication years  # Store results for report results_list <- list(abc = diverse_results)  # Store visualization paths visualizations <- list(   heatmap = \"migraine_heatmap.png\",   network = \"migraine_network.html\",   chord = \"migraine_chord.html\" )  # Create comprehensive report gen_report(   results_list = results_list,   visualizations = visualizations,   articles = articles_with_years,   output_file = \"migraine_discoveries.html\" ) #> Generated comprehensive report: migraine_discoveries.html  cat(\"\\nDiscovery analysis complete!\\n\") #>  #> Discovery analysis complete!"},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"interactive-visualizations-and-report","dir":"Articles","previous_headings":"Introduction","what":"Interactive Visualizations and Report","title":"Getting Started with Literature-Based Discovery","text":"LBDiscover package generates interactive visualizations comprehensive report. can see embedded report interactive visualizations.","code":""},{"path":[]},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"direct-links-to-interactive-visualizations","dir":"Articles","previous_headings":"Introduction > Interactive Visualizations and Report","what":"Direct Links to Interactive Visualizations","title":"Getting Started with Literature-Based Discovery","text":"interactive visualizations embedded :","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"advanced-features","dir":"Articles","previous_headings":"Introduction","what":"Advanced Features","title":"Getting Started with Literature-Based Discovery","text":"LBDiscover package offers several advanced features ’ve demonstrated example: Term variation detection: Automatically finding different forms primary term interest Custom dictionary integration: Creating using custom dictionaries alongside standard ones Entity validation: Filtering entities ensure biomedical relevance Improved ABC model: Enhanced scoring methods filtering options Statistical validation: Applying rigorous statistical tests potential discoveries Result diversification: Ensuring diverse set discovery candidates Interactive visualizations: Creating dynamic network chord diagrams Evidence evaluation: Assessing literature support discoveries Comprehensive reporting: Generating detailed HTML reports findings","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Intro_to_Literature-Based_Discovery.html","id":"conclusion","dir":"Articles","previous_headings":"Introduction","what":"Conclusion","title":"Getting Started with Literature-Based Discovery","text":"vignette demonstrated comprehensive workflow literature-based discovery using LBDiscover package. improved ABC model additional utility functions provide robust framework identifying potential novel connections biomedical literature. Users can explore examples included inst\\examples folder.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"text-preprocessing-and-entity-extraction","dir":"Articles","previous_headings":"","what":"Text Preprocessing and Entity Extraction","title":"Text Preprocessing and Entity Extraction","text":"vignette explains text preprocessing entity extraction capabilities LBDiscover package, fundamental steps literature-based discovery process.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"introduction","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Introduction","title":"Text Preprocessing and Entity Extraction","text":"applying discovery models, need preprocess text data extract entities interest. steps transform raw text structured information can used discovering relationships biomedical concepts.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"loading-the-package","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Loading the Package","title":"Text Preprocessing and Entity Extraction","text":"","code":"library(LBDiscover) #> Loading LBDiscover package"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"data-retrieval","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Data Retrieval","title":"Text Preprocessing and Entity Extraction","text":"First, let’s retrieve sample articles:","code":"# Search for articles about migraines migraine_articles <- pubmed_search(   query = \"migraine pathophysiology\",   max_results = 100 ) #> Created pubmed_cache environment for result caching #> Searching PubMed for: migraine pathophysiology #> Found 11643 results, retrieving 100 records #> Fetching batch 1 of 1 (records 1-100) #> Processing 100 articles #>   Processing article 100 of 100 #> Cached search results for future use  # View the first article head(migraine_articles[, c(\"pmid\", \"title\")], 3) #>       pmid #> 1 40371864 #> 2 40357069 #> 3 40354987 #>                                                                                                                         title #> 1 [Morning headaches in patients with obstructive sleep apnea syndrome: Pathogenesis, differential diagnosis, and treatment]. #> 2             Third Occipital Nerve Block and Cooled Radiofrequency Ablation for Managing Hemicrania Continua: A Case Report. #> 3                     Hypothalamic connectivity strength is decreasing with polygenic risk in migraine without aura patients."},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"basic-text-preprocessing","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Basic Text Preprocessing","title":"Text Preprocessing and Entity Extraction","text":"first step preprocess text data extract meaningful terms:","code":"# Preprocess the abstracts preprocessed_data <- preprocess_text(   migraine_articles,   text_column = \"abstract\",   remove_stopwords = TRUE,   custom_stopwords = c(\"study\", \"patient\", \"result\", \"conclusion\"),   min_word_length = 3,   max_word_length = 25 ) #> Tokenizing text...  # View terms extracted from the first document head(preprocessed_data$terms[[1]], 10) #>          word count #> 1   abdominal     1 #> 2   addressed     1 #> 3      airway     1 #> 4       algic     1 #> 5       among     1 #> 6  anatomical     1 #> 7       apnea     3 #> 8  associated     6 #> 9   attention     1 #> 10       back     1"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"optimized-preprocessing-for-large-datasets","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Basic Text Preprocessing","what":"Optimized Preprocessing for Large Datasets","title":"Text Preprocessing and Entity Extraction","text":"larger datasets, can use optimized vectorized preprocessing function:","code":"# Use optimized vectorized preprocessing opt_preprocessed_data <- vec_preprocess(   migraine_articles,   text_column = \"abstract\",   remove_stopwords = TRUE,   min_word_length = 3,   chunk_size = 50  # Process in chunks of 50 documents ) #> Processing text in 2 chunks... #>   |                                                                              |                                                                      |   0%  |                                                                              |===================================                                   |  50%  |                                                                              |======================================================================| 100%  # Compare processing times system.time({   preprocess_text(     migraine_articles,     text_column = \"abstract\",     remove_stopwords = TRUE   ) }) #> Tokenizing text... #>    user  system elapsed  #>   0.065   0.000   0.064  system.time({   vec_preprocess(     migraine_articles,     text_column = \"abstract\",     remove_stopwords = TRUE,     chunk_size = 50   ) }) #> Processing text in 2 chunks... #>   |                                                                              |                                                                      |   0%  |                                                                              |===================================                                   |  50%  |                                                                              |======================================================================| 100% #>    user  system elapsed  #>    0.06    0.00    0.06"},{"path":[]},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"n-gram-extraction","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Advanced Text Analysis","what":"N-gram Extraction","title":"Text Preprocessing and Entity Extraction","text":"can extract n-grams (sequences n words) capture multi-word concepts:","code":"# Extract bigrams (2-word sequences) bigrams <- extract_ngrams(   migraine_articles$abstract,   n = 2,   min_freq = 2 )  # View the most frequent bigrams head(bigrams, 10) #>                 ngram frequency #> 7529           in the       161 #> 10138     of migraine        79 #> 10239          of the        79 #> 10631             p 0        79 #> 10987   patients with        68 #> 16211   with migraine        58 #> 7452      in migraine        51 #> 2828  associated with        48 #> 14627      this study        47 #> 14908          to the        44"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"sentence-segmentation","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Advanced Text Analysis","what":"Sentence Segmentation","title":"Text Preprocessing and Entity Extraction","text":"Segmenting text sentences can useful granular analysis:","code":"# Extract sentences from the first abstract abstracts <- migraine_articles$abstract first_abstract <- abstracts[1]  # Make sure we have a valid abstract if(is.na(first_abstract) || length(first_abstract) == 0 || nchar(first_abstract) == 0) {   # Find the first non-empty abstract   valid_idx <- which(!is.na(abstracts) & nchar(abstracts) > 0)   if(length(valid_idx) > 0) {     first_abstract <- abstracts[valid_idx[1]]     cat(\"First abstract was empty, using abstract #\", valid_idx[1], \"instead.\\n\")   } else {     # Create a sample abstract for demonstration     first_abstract <- \"This is a sample abstract for demonstration. It contains multiple sentences. Each sentence will be extracted separately.\"     cat(\"No valid abstracts found. Using a sample abstract for demonstration.\\n\")   } }  # Now segment the valid abstract sentences <- segment_sentences(first_abstract)  # Check if sentences list has elements before trying to access them if(length(sentences) > 0 && length(sentences[[1]]) > 0) {   # View the first few sentences   head(sentences[[1]], min(3, length(sentences[[1]]))) } else {   cat(\"No sentences could be extracted. The abstract might be too short or formatted incorrectly.\\n\") } #> [1] \"Sleep disorders are often associated with painful ones, especially in patients with chronic syndromes.\"                                      #> [2] \"Cephalgia occupies an important place among such algic forms as fibromyalgia, back pain, and abdominal and joint pain.\"                      #> [3] \"Headaches and sleep disturbances may be independent, deri.e. from a single pathogenetic factor, or their relationship may be bidi.e.tional.\"  # View the first few sentences head(sentences[[1]], 3) #> [1] \"Sleep disorders are often associated with painful ones, especially in patients with chronic syndromes.\"                                      #> [2] \"Cephalgia occupies an important place among such algic forms as fibromyalgia, back pain, and abdominal and joint pain.\"                      #> [3] \"Headaches and sleep disturbances may be independent, deri.e. from a single pathogenetic factor, or their relationship may be bidi.e.tional.\""},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"language-detection","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Advanced Text Analysis","what":"Language Detection","title":"Text Preprocessing and Entity Extraction","text":"dealing multilingual corpora, can detect language document:","code":"# Filter out NA values from abstracts and detect language abstracts <- migraine_articles$abstract[1:5] valid_abstracts <- abstracts[!is.na(abstracts)]  # Apply language detection to valid abstracts if (length(valid_abstracts) > 0) {   languages <- sapply(valid_abstracts, detect_lang)      # View results   data.frame(     abstract_id = which(!is.na(abstracts)),     language = languages   ) } else {   message(\"No valid abstracts found for language detection\") } #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         abstract_id #> Sleep disorders are often associated with painful ones, especially in patients with chronic syndromes. Cephalgia occupies an important place among such algic forms as fibromyalgia, back pain, and abdominal and joint pain. Headaches and sleep disturbances may be independent, derived from a single pathogenetic factor, or their relationship may be bidirectional. This review focuses on the reciprocal relationship between headaches and sleep disorders; particular attention is paid to the morning headache variant associated with obstructive sleep apnea (OSA). Modern data on the anatomical structures and pathophysiological mechanisms common to disorders in the regulation of the sleep-wake cycle and the perception of pain impulses are presented. Possible pathogenetic processes and nuances of differential diagnosis of headaches associated with sleep apnea are discussed. Methods of treating headaches associated with sleep disorders, in particular OSA-associated cephalgia, are addressed. The effectiveness of therapy with constant positive airway pressure (CPAP therapy) and splint therapy for OSA has been shown for headaches associated with sleep apnea.                                                                                                                                                                                                                           1 #> Hemicrania continua is a rare and debilitating headache disorder characterized by continuous, unilateral pain that responds to indomethacin but is often resistant to other treatments. This report presents the case of a 26-year-old female patient with refractory hemicrania continua and chronic migraine who achieved significant pain relief following a fluoroscopy-guided third occipital nerve block and subsequent radiofrequency ablation (RFA) of the C2-C3 facet joint. The procedure resulted in an immediate reduction of pain as evident from a reduction in the Visual Analog Scale (VAS) score from 10/10 to 0/10, with sustained relief (VAS 2/10) at three months and notable improvement in the patient's quality of life. This case highlights the potential efficacy of targeting the third occipital nerve for the management of hemicrania continua, with thermal RFA (COOLIEF, Avanos Medical, Inc., Alpharetta, GA) offering prolonged relief by ablating nociceptive fibers. Given the emerging evidence supporting the involvement of the third occipital nerve in headache pathophysiology, the third occipital nerve block and RFA represent promising strategies for refractory cases.                                                                                                                                                                                                           2 #> Migraine is a heritable primary headache disorder which pathophysiology involves altered hypothalamic activity during migraine attacks. To explore the relationship between hypothalamic functional connectivity (HYPT FC) and genetic predisposition characterised by polygenic risk scores (PRS), in migraine, this research examines two types of PRS: one based on all migraine patients (PRSALL) regardless their time of diagnosis and other disorders, and another on \"migraine-first\" patients (PRSFIRST), whose first diagnosed condition was migraine in their lifetime. In an independent sample of 35 migraine patients and 38 healthy controls, using resting-state functional magnetic resonance (rfMRI, 3T) brain imaging, the study reveals significant hypoconnectivity of hypothalamus with the two investigated PRS scores but with different brain areas. While weakened hypothalamic connections in relations with PRSALL highlight regions involved in pain modulation, correlation with PRSFIRST emphasizes decreased connections with sensory and integrative brain areas, suggesting a link between migraine-first genetic risk and cortical hyperexcitability. Our results demonstrate that the polygenic risk of different migraine subgroups may advance our insight into the specific genetic and neural underpinnings of migraine, advancing precision medicine approaches in this field.           3 #> Like Janus, the Roman god of beginnings, transitions, and endings, spreading depolarizations (SDs) can be depicted with two faces: one looking backward, waving a symbolic farewell to the end of a cortical seizure; the other forward looking, opening a darker door for a fatal wave in the brainstem that ends life. There is good agreement on the distinct electrical nature of both events, but neither role is yet proven in patients. SD is a slow-moving wave of cellular depolarization that steadily silences neuronal networks and depresses EEG amplitude, whereas seizures represent fast, intermittent synchronization of neural networks with highly variable EEG activation patterns. However, the thresholds triggering both events are neither fixed nor inseparable; indeed, their co-occurrence and interaction depend on dimly-lit intrinsic brain pathophysiology. New insights into single gene control of SD and seizure thresholds are beginning to illuminate the darkness. Here, we review recent data and consider the title's question at the end.                                                                                                                                                                                                                                                                                                                                                 4 #> The article addresses the main mechanisms of migraine pathogenesis in terms of biochemical features (neurotransmitter metabolism, neurochemistry, neurophysiology, and neurogenetics). The effect of hormones, electrolytes (magnesium, calcium, sodium), vitamins (vitamin D, B12), and other biologically active molecules (melatonin, L-carnitine, L-tryptophan) on the course of the disease is considered. Including some laboratory tests in the migraine diagnostic algorithm helps identify the secondary nature of headache and/or dizziness, manage therapeutic approaches, and adjust the prognosis and treatment outcomes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            5 #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         language #> Sleep disorders are often associated with painful ones, especially in patients with chronic syndromes. Cephalgia occupies an important place among such algic forms as fibromyalgia, back pain, and abdominal and joint pain. Headaches and sleep disturbances may be independent, derived from a single pathogenetic factor, or their relationship may be bidirectional. This review focuses on the reciprocal relationship between headaches and sleep disorders; particular attention is paid to the morning headache variant associated with obstructive sleep apnea (OSA). Modern data on the anatomical structures and pathophysiological mechanisms common to disorders in the regulation of the sleep-wake cycle and the perception of pain impulses are presented. Possible pathogenetic processes and nuances of differential diagnosis of headaches associated with sleep apnea are discussed. Methods of treating headaches associated with sleep disorders, in particular OSA-associated cephalgia, are addressed. The effectiveness of therapy with constant positive airway pressure (CPAP therapy) and splint therapy for OSA has been shown for headaches associated with sleep apnea.                                                                                                                                                                                                                       en #> Hemicrania continua is a rare and debilitating headache disorder characterized by continuous, unilateral pain that responds to indomethacin but is often resistant to other treatments. This report presents the case of a 26-year-old female patient with refractory hemicrania continua and chronic migraine who achieved significant pain relief following a fluoroscopy-guided third occipital nerve block and subsequent radiofrequency ablation (RFA) of the C2-C3 facet joint. The procedure resulted in an immediate reduction of pain as evident from a reduction in the Visual Analog Scale (VAS) score from 10/10 to 0/10, with sustained relief (VAS 2/10) at three months and notable improvement in the patient's quality of life. This case highlights the potential efficacy of targeting the third occipital nerve for the management of hemicrania continua, with thermal RFA (COOLIEF, Avanos Medical, Inc., Alpharetta, GA) offering prolonged relief by ablating nociceptive fibers. Given the emerging evidence supporting the involvement of the third occipital nerve in headache pathophysiology, the third occipital nerve block and RFA represent promising strategies for refractory cases.                                                                                                                                                                                                       en #> Migraine is a heritable primary headache disorder which pathophysiology involves altered hypothalamic activity during migraine attacks. To explore the relationship between hypothalamic functional connectivity (HYPT FC) and genetic predisposition characterised by polygenic risk scores (PRS), in migraine, this research examines two types of PRS: one based on all migraine patients (PRSALL) regardless their time of diagnosis and other disorders, and another on \"migraine-first\" patients (PRSFIRST), whose first diagnosed condition was migraine in their lifetime. In an independent sample of 35 migraine patients and 38 healthy controls, using resting-state functional magnetic resonance (rfMRI, 3T) brain imaging, the study reveals significant hypoconnectivity of hypothalamus with the two investigated PRS scores but with different brain areas. While weakened hypothalamic connections in relations with PRSALL highlight regions involved in pain modulation, correlation with PRSFIRST emphasizes decreased connections with sensory and integrative brain areas, suggesting a link between migraine-first genetic risk and cortical hyperexcitability. Our results demonstrate that the polygenic risk of different migraine subgroups may advance our insight into the specific genetic and neural underpinnings of migraine, advancing precision medicine approaches in this field.       en #> Like Janus, the Roman god of beginnings, transitions, and endings, spreading depolarizations (SDs) can be depicted with two faces: one looking backward, waving a symbolic farewell to the end of a cortical seizure; the other forward looking, opening a darker door for a fatal wave in the brainstem that ends life. There is good agreement on the distinct electrical nature of both events, but neither role is yet proven in patients. SD is a slow-moving wave of cellular depolarization that steadily silences neuronal networks and depresses EEG amplitude, whereas seizures represent fast, intermittent synchronization of neural networks with highly variable EEG activation patterns. However, the thresholds triggering both events are neither fixed nor inseparable; indeed, their co-occurrence and interaction depend on dimly-lit intrinsic brain pathophysiology. New insights into single gene control of SD and seizure thresholds are beginning to illuminate the darkness. Here, we review recent data and consider the title's question at the end.                                                                                                                                                                                                                                                                                                                                             en #> The article addresses the main mechanisms of migraine pathogenesis in terms of biochemical features (neurotransmitter metabolism, neurochemistry, neurophysiology, and neurogenetics). The effect of hormones, electrolytes (magnesium, calcium, sodium), vitamins (vitamin D, B12), and other biologically active molecules (melatonin, L-carnitine, L-tryptophan) on the course of the disease is considered. Including some laboratory tests in the migraine diagnostic algorithm helps identify the secondary nature of headache and/or dizziness, manage therapeutic approaches, and adjust the prognosis and treatment outcomes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        en"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"entity-extraction","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Entity Extraction","title":"Text Preprocessing and Entity Extraction","text":"preprocessing, next step extract biomedical entities text.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"loading-entity-dictionaries","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Entity Extraction","what":"Loading Entity Dictionaries","title":"Text Preprocessing and Entity Extraction","text":"First, let’s load entity dictionaries used entity recognition:","code":"# Load a disease dictionary disease_dict <- load_dictionary(   dictionary_type = \"disease\",   source = \"mesh\" ) #> Searching MeSH database for: disease[MeSH] #> Found 193563 PubMed records with matching MeSH terms #> Error getting MeSH links: Must specify either (not both) 'id' or 'web_history' arguments #> Trying direct MeSH database search... #> Processing batch 1 of 5 #> Extracted 20 unique terms from MeSH text format #> Processing batch 2 of 5 #> Extracted 21 unique terms from MeSH text format #> Processing batch 3 of 5 #> Extracted 25 unique terms from MeSH text format #> Processing batch 4 of 5 #> Extracted 19 unique terms from MeSH text format #> Processing batch 5 of 5 #> Extracted 21 unique terms from MeSH text format #> Retrieved 102 unique terms from MeSH #> Sanitizing dictionary with 102 terms... #>   Removed 56 terms that did not match their claimed entity types #>   Removed 38 terms with non-alphanumeric characters (final cleanup) #> Sanitization complete. 8 terms remaining (7.8% of original)  # Load a drug dictionary drug_dict <- load_dictionary(   dictionary_type = \"drug\",   source = \"mesh\" ) #> Searching MeSH database for: pharmaceutical preparations[MeSH] #> Found 985543 PubMed records with matching MeSH terms #> Error getting MeSH links: Must specify either (not both) 'id' or 'web_history' arguments #> Trying direct MeSH database search... #> Processing batch 1 of 2 #> Extracted 23 unique terms from MeSH text format #> Processing batch 2 of 2 #> Extracted 3 unique terms from MeSH text format #> Retrieved 26 unique terms from MeSH #> Sanitizing dictionary with 26 terms... #>   Removed 26 terms that did not match their claimed entity types #> Sanitization complete. 0 terms remaining (0% of original)  # View a sample of each dictionary head(disease_dict, 3) #>               term            id    type    source #> 10     Lobomycosis       MESH_10 disease mesh_text #> 20         Disease MESH_ENTRY_19 disease mesh_text #> 48 Osteochondrosis        MESH_7 disease mesh_text head(drug_dict, 3) #> [1] term   id     type   source #> <0 rows> (or 0-length row.names)"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"basic-entity-extraction","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Entity Extraction","what":"Basic Entity Extraction","title":"Text Preprocessing and Entity Extraction","text":"Now can extract entities text using dictionaries:","code":"# Extract disease and drug entities entities <- extract_entities(   preprocessed_data,   text_column = \"abstract\",   dictionary = rbind(disease_dict, drug_dict),   case_sensitive = FALSE,   overlap_strategy = \"priority\" ) #> Sanitizing dictionary with 8 terms... #> Sanitization complete. 8 terms remaining (100% of original) #> Extracting entities from 98 documents... #> Extracted 36 entity mentions: #>   disease: 36  # View some extracted entities head(entities[, c(\"doc_id\", \"entity\", \"entity_type\", \"sentence\")], 10) #>    doc_id  entity entity_type #> 1       5 Disease     disease #> 2      10 Disease     disease #> 3      22 Disease     disease #> 4      22 Disease     disease #> 5      31 Disease     disease #> 6      41 Disease     disease #> 7      44 Disease     disease #> 8      45 Disease     disease #> 9      47 Disease     disease #> 10     54 Disease     disease #>                                                                                                                                                                                                                                 sentence #> 1               The effect of hormones, electrolytes (magnesium, calcium, sodium), vitamins (vitamin D, B12), and other biologically active molecules (melatonin, L-carnitine, L-tryptophan) on the course of the disease is considered. #> 2                                                The the prevalence of aura (p = 0.028), age (p = 0.001) and mean disease duration (p < 0.001, t=-4.257) were significantly higher in migraine patients with WMH than those without WMH. #> 3                                                                                                                                                       Neuroimaging has contributed to a better understanding of VSS disease mechanism. #> 4                                                                                       Given the complexity of its disease state, multidisciplinary therapeutic approaches appear to be required for more effective symptom management. #> 5                                                                                 Ménière's disease (4%) and vestibular neuritis/labyrinthitis (3.9%) were associated with younger patients and unilateral or asymmetrical hearing loss. #> 6  Therefore, although migraine with and without aura are considered two types of the same disease, more research should focus on their differences, thus finally enabling better specific treatment options for both types of migraine. #> 7                                                                                                                                                      Common comorbidities were hypertension, diabetes, and polycystic ovarian disease. #> 8                                                             CONCLUSIONS: Our study shows that UFs share substantial genetic basis with traits related to BP, obesity, diabetes, and migraine, a predominantly female vascular disease. #> 9                  BACKGROUND: Coronavirus disease 2019 (COVID-19), caused by the SARS-CoV-2 virus, placed unprecedented pressure on public health systems due to its mortality and global panic-and later due to long COVID challenges. #> 10                        Central nervous system (CNS) disorders, such as Alzheimer's disease (AD), Parkinson's disease (PD), multiple sclerosis (MS), and migraines, rank among the most prevalent and concerning conditions worldwide."},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"complete-entity-extraction-workflow","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Entity Extraction","what":"Complete Entity Extraction Workflow","title":"Text Preprocessing and Entity Extraction","text":"comprehensive approach, can use complete entity extraction workflow:","code":"# Extract entities using the complete workflow # Check if running in R CMD check environment is_check <- !interactive() &&              (!is.null(Sys.getenv(\"R_CHECK_RUNNING\")) &&               Sys.getenv(\"R_CHECK_RUNNING\") == \"true\")               # More robust check for testing environment if (!is_check && !is.null(Sys.getenv(\"_R_CHECK_LIMIT_CORES_\"))) {   is_check <- TRUE }  # Set number of cores based on environment num_cores_to_use <- if(is_check) 1 else 4  # Extract entities using the complete workflow entities_workflow <- extract_entities_workflow(   preprocessed_data,   text_column = \"abstract\",   entity_types = c(\"disease\", \"drug\", \"gene\", \"protein\", \"pathway\"),   dictionary_sources = c(\"local\", \"mesh\"),   sanitize = TRUE,   parallel = !is_check,           # Disable parallel in check environment   num_cores = num_cores_to_use    # Use 1 core in check environment ) #> Running in R CMD check environment. Disabling parallel processing. #> Creating dictionaries for entity extraction... #> Loading dictionaries sequentially... #> Package not installed or dictionary not found. Using example dictionaries. #> Creating dummy dictionary for disease #>   Added 20 terms from disease (local) #> Package not installed or dictionary not found. Using example dictionaries. #> Creating dummy dictionary for drug #>   Added 20 terms from drug (local) #> Package not installed or dictionary not found. Using example dictionaries. #> Creating dummy dictionary for gene #>   Added 20 terms from gene (local) #> Searching MeSH database for: proteins[MeSH] #> Found 7579035 PubMed records with matching MeSH terms #> Error getting MeSH links: Must specify either (not both) 'id' or 'web_history' arguments #> Trying direct MeSH database search... #> Processing batch 1 of 5 #> Extracted 24 unique terms from MeSH text format #> Processing batch 2 of 5 #> Extracted 23 unique terms from MeSH text format #> Processing batch 3 of 5 #> Extracted 19 unique terms from MeSH text format #> Processing batch 4 of 5 #> Extracted 20 unique terms from MeSH text format #> Processing batch 5 of 5 #> Extracted 19 unique terms from MeSH text format #> Retrieved 105 unique terms from MeSH #>   Added 105 terms from protein (mesh) #> Searching MeSH database for: metabolic networks and pathways[MeSH] #> Found 184983 PubMed records with matching MeSH terms #> Error getting MeSH links: Must specify either (not both) 'id' or 'web_history' arguments #> Trying direct MeSH database search... #> Processing batch 1 of 1 #> Extracted 6 unique terms from MeSH text format #> Retrieved 6 unique terms from MeSH #>   Added 6 terms from pathway (mesh) #> Created combined dictionary with 171 unique terms #> Sanitizing dictionary with 171 terms... #>   Removed 8 terms with numbers followed by special characters #>   Correcting type for 'headache' from 'disease' to 'symptom' #>   Correcting type for 'fatigue' from 'disease' to 'symptom' #>   Applied 2 type corrections for commonly misclassified terms #>   Removed 107 terms that did not match their claimed entity types #>   Removed 42 terms with non-alphanumeric characters (final cleanup) #> Sanitization complete. 14 terms remaining (8.2% of original) #> Extracting entities from 98 documents... #> Processing batch 1/1 #> Extracting entities from 98 documents... #> Extracted 662 entity mentions: #>   disease: 533 #>   protein: 13 #>   symptom: 116 #> Extracted 662 entity mentions in 0.14 minutes #>   disease: 533 #>   protein: 13 #>   symptom: 116  # View summary of entity types table(entities_workflow$entity_type) #>  #> disease protein symptom  #>     533      13     116"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"customizing-entity-extraction","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction > Entity Extraction","what":"Customizing Entity Extraction","title":"Text Preprocessing and Entity Extraction","text":"can customize entity extraction process providing additional MeSH queries custom dictionaries:","code":"# Define custom MeSH queries for different entity types mesh_queries <- list(   \"disease\" = \"migraine disorders[MeSH] OR headache disorders[MeSH]\",   \"drug\" = \"analgesics[MeSH] OR serotonin agonists[MeSH] OR anticonvulsants[MeSH]\",   \"gene\" = \"genes[MeSH] OR channelopathy[MeSH]\" )  # Create a custom dictionary custom_dict <- data.frame(   term = c(\"CGRP\", \"trigeminal nerve\", \"cortical spreading depression\"),   type = c(\"protein\", \"anatomy\", \"biological_process\"),   id = c(\"CUSTOM_1\", \"CUSTOM_2\", \"CUSTOM_3\"),   source = rep(\"custom\", 3),   stringsAsFactors = FALSE )  # Extract entities with custom settings custom_entities <- extract_entities_workflow(   preprocessed_data,   text_column = \"abstract\",   entity_types = c(\"disease\", \"drug\", \"gene\", \"protein\", \"pathway\"),   dictionary_sources = c(\"local\", \"mesh\"),   additional_mesh_queries = mesh_queries,   custom_dictionary = custom_dict,   sanitize = TRUE ) #> Running in R CMD check environment. Disabling parallel processing. #> Creating dictionaries for entity extraction... #> Adding 3 terms from custom dictionary #> Loading dictionaries sequentially... #>   Using cached dictionary for disease (local) #>   Using cached dictionary for drug (local) #>   Using cached dictionary for gene (local) #>   Using cached dictionary for protein (mesh) #>   Using cached dictionary for pathway (mesh) #> Created combined dictionary with 174 unique terms #> Sanitizing dictionary with 171 terms... #>   Removed 8 terms with numbers followed by special characters #>   Correcting type for 'headache' from 'disease' to 'symptom' #>   Correcting type for 'fatigue' from 'disease' to 'symptom' #>   Applied 2 type corrections for commonly misclassified terms #>   Removed 107 terms that did not match their claimed entity types #>   Removed 42 terms with non-alphanumeric characters (final cleanup) #> Sanitization complete. 14 terms remaining (8.2% of original) #> Extracting entities from 98 documents... #> Processing batch 1/1 #> Extracting entities from 98 documents... #> Extracted 736 entity mentions: #>   anatomy: 2 #>   biological_process: 8 #>   disease: 533 #>   protein: 77 #>   symptom: 116 #> Extracted 736 entity mentions in 0.01 minutes #>   anatomy: 2 #>   biological_process: 8 #>   disease: 533 #>   protein: 77 #>   symptom: 116  # View custom entities custom_entities[custom_entities$source == \"custom\", ] #> [1] entity      entity_type doc_id      start_pos   end_pos     sentence    #> [7] frequency   #> <0 rows> (or 0-length row.names)"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"dictionary-sanitization","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Dictionary Sanitization","title":"Text Preprocessing and Entity Extraction","text":"quality entity extraction heavily depends quality dictionaries. can sanitize dictionaries improve extraction quality:","code":"# Create a raw dictionary with some problematic entries raw_dict <- data.frame(   term = c(\"migraine\", \"5-HT\", \"headache\", \"the\", \"and\", \"patient\", \"inflammation\", \"study\"),   type = c(\"disease\", \"chemical\", \"symptom\", \"NA\", \"NA\", \"NA\", \"biological_process\", \"NA\"),   id = paste0(\"ID_\", 1:8),   source = rep(\"example\", 8),   stringsAsFactors = FALSE )  # Sanitize the dictionary sanitized_dict <- sanitize_dictionary(   raw_dict,   term_column = \"term\",   type_column = \"type\",   validate_types = TRUE,   verbose = TRUE ) #> Sanitizing dictionary with 8 terms... #>   Removed 1 terms with numbers followed by special characters #>   Removed 1 common non-medical terms, conjunctive adverbs, and general terms #> Sanitization complete. 6 terms remaining (75% of original)  # View the sanitized dictionary sanitized_dict #>           term               type   id  source #> 1     migraine            disease ID_1 example #> 3     headache            symptom ID_3 example #> 4          the                 NA ID_4 example #> 5          and                 NA ID_5 example #> 7 inflammation biological_process ID_7 example #> 8        study                 NA ID_8 example"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"mapping-terms-to-biomedical-ontologies","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Mapping Terms to Biomedical Ontologies","title":"Text Preprocessing and Entity Extraction","text":"can map extracted terms standard biomedical ontologies like MeSH UMLS:","code":"# Extract terms to map terms_to_map <- c(\"migraine\", \"headache\", \"CGRP\", \"serotonin\")  # Map to MeSH mesh_mappings <- map_ontology(   terms_to_map,   ontology = \"mesh\",   fuzzy_match = TRUE,   similarity_threshold = 0.8 ) #> Searching MeSH database for: disease[MeSH] #> Found 193563 PubMed records with matching MeSH terms #> Error getting MeSH links: Must specify either (not both) 'id' or 'web_history' arguments #> Trying direct MeSH database search... #> Processing batch 1 of 5 #> Extracted 20 unique terms from MeSH text format #> Processing batch 2 of 5 #> Extracted 21 unique terms from MeSH text format #> Processing batch 3 of 5 #> Extracted 25 unique terms from MeSH text format #> Processing batch 4 of 5 #> Extracted 19 unique terms from MeSH text format #> Processing batch 5 of 5 #> Extracted 21 unique terms from MeSH text format #> Retrieved 102 unique terms from MeSH #> Sanitizing dictionary with 102 terms... #>   Removed 56 terms that did not match their claimed entity types #>   Removed 38 terms with non-alphanumeric characters (final cleanup) #> Sanitization complete. 8 terms remaining (7.8% of original) #> No matches found for the input terms in the mesh ontology  # View MeSH mappings mesh_mappings #> [1] term          ontology_id   ontology_term match_type    #> <0 rows> (or 0-length row.names)"},{"path":"https://liu-chao.site/LBDiscover/articles/Text_Preprocessing.html","id":"topic-modeling","dir":"Articles","previous_headings":"Text Preprocessing and Entity Extraction","what":"Topic Modeling","title":"Text Preprocessing and Entity Extraction","text":"can also apply topic modeling discover main themes corpus:","code":"# Extract topics from the corpus topics <- extract_topics(   migraine_articles,   text_column = \"abstract\",   n_topics = 5,   max_terms = 10 ) #> Tokenizing text...  # View top terms for each topic topics$topics #> $`Topic 1` #>                term    weight #> migraine   migraine 158.54694 #> cgrp           cgrp  33.93941 #> aura           aura  28.48012 #> headache   headache  26.85610 #> pain           pain  26.52241 #> between     between  23.21742 #> patients   patients  20.81965 #> study         study  20.37030 #> related     related  20.09579 #> treatment treatment  19.25877 #>  #> $`Topic 2` #>                    term    weight #> progression progression 37.439073 #> migraine       migraine 36.013245 #> 634                 634 10.570779 #> definitions definitions 10.570779 #> midas             midas  9.241191 #> mhd                 mhd  8.808983 #> increase       increase  7.308507 #> odds               odds  7.052829 #> year               year  7.047243 #> definition   definition  7.047186 #>  #> $`Topic 3` #>                    term   weight #> patients       patients 44.09683 #> brain             brain 40.43149 #> asd                 asd 39.81409 #> mwoa               mwoa 39.58473 #> stroke           stroke 34.74475 #> after             after 31.86060 #> network         network 29.71455 #> significant significant 28.73627 #> compared       compared 26.71239 #> individuals individuals 25.19733 #>  #> $`Topic 4` #>                    term   weight #> migraine       migraine 73.11895 #> covid             covid 60.29671 #> patients       patients 48.10263 #> long               long 33.81408 #> individuals individuals 31.86965 #> headache       headache 27.79943 #> symptoms       symptoms 25.11927 #> without         without 20.33448 #> sex                 sex 15.35365 #> study             study 15.23040 #>  #> $`Topic 5` #>                      term   weight #> migraine         migraine 78.23895 #> group               group 28.04809 #> patients         patients 24.72097 #> study               study 16.40871 #> between           between 15.56503 #> edema               edema 11.28163 #> perilesional perilesional 11.28163 #> days                 days 11.08811 #> scores             scores 10.27279 #> compared         compared 10.20141"},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"discovery-models-in-literature-based-discovery","dir":"Articles","previous_headings":"","what":"Discovery Models in Literature-Based Discovery","title":"Working with Discovery Models","text":"vignette explores various discovery models available LBDiscover package, enable identify hidden connections biomedical literature.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"overview-of-discovery-models","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery","what":"Overview of Discovery Models","title":"Working with Discovery Models","text":"LBDiscover package implements several discovery models: ABC Model: classic model discovers implicit connections terms C intermediate B terms. AnC Model: variant ABC model uses multiple B terms establish stronger connections C. BITOLA Model: model incorporates semantic types guide discovery process. LSI (Latent Semantic Indexing) Model: model uses dimensionality reduction discover semantically related terms. model strengths suitable different types discovery tasks.","code":""},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"setup-and-data-preparation","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery","what":"Setup and Data Preparation","title":"Working with Discovery Models","text":"Let’s start loading package preparing sample data:","code":"library(LBDiscover) # Search for articles about Alzheimer's disease alzheimer_articles <- pubmed_search(   query = \"alzheimer pathophysiology\",   max_results = 1000 )  # Search for treatment-related articles drug_articles <- pubmed_search(   query = \"neurodegenerative disease treatment OR cognitive impairment therapy\",   max_results = 1000 )  # Combine and remove duplicates all_articles <- merge_results(alzheimer_articles, drug_articles) print(paste(\"Retrieved\", nrow(all_articles), \"unique articles\")) #> [1] \"Retrieved 1955 unique articles\"  # Preprocess text preprocessed_articles <- preprocess_text(   all_articles,   text_column = \"abstract\",   remove_stopwords = TRUE,   min_word_length = 3 )  # Extract entities entities <- extract_entities_workflow(   preprocessed_articles,   text_column = \"abstract\",   entity_types = c(\"disease\", \"drug\", \"gene\", \"protein\", \"chemical\", \"pathway\"),   dictionary_sources = c(\"local\", \"mesh\") )  # Create co-occurrence matrix with less restrictive normalization co_matrix <- create_comat(   entities,   doc_id_col = \"doc_id\",   entity_col = \"entity\",   type_col = \"entity_type\",   normalize = TRUE,   normalization_method = \"dice\"  # Using dice coefficient for better sensitivity )  # Define our primary term of interest a_term <- \"alzheimer\"  # Let's first examine what terms are available in the co-occurrence matrix available_terms <- rownames(co_matrix) message(\"First 30 terms in the co-occurrence matrix:\") print(head(available_terms, 30)) #>  [1] \"Receptors\"      \"Acid\"           \"migraine\"       \"fatigue\"        #>  [5] \"cancer\"         \"Toxins\"         \"Nitric\"         \"Menopause\"      #>  [9] \"headache\"       \"Aromatherapy\"   \"heparin\"        \"Taste\"          #> [13] \"hypoxemia\"      \"Phytochemicals\" \"Anesthetics\"    \"Hemolysis\"      #> [17] \"thrombosis\"     \"Pleurodesis\"  # Check if the term is misspelled or slightly different message(\"\\nChecking for variations of 'alzheimer' and related terms:\") alzheimer_like <- grep(\"alzheim|dementia|cognitive\", available_terms, value = TRUE, ignore.case = TRUE) print(alzheimer_like) #> character(0)  # Find our primary term in the co-occurrence matrix with flexible matching if (!a_term %in% rownames(co_matrix)) {   # First, try to find terms containing 'alzheimer'   possible_matches <- alzheimer_like      if (length(possible_matches) > 0) {     message(\"Primary term '\", a_term, \"' not found exactly. Using related term: \", possible_matches[1])     a_term <- possible_matches[1]   } else {     # If no Alzheimer terms, try some other neurodegenerative disease terms     neuro_terms <- grep(\"neurodegenerat|parkinson|huntington|dementia\", rownames(co_matrix), value = TRUE, ignore.case = TRUE)     if (length(neuro_terms) > 0) {       message(\"No Alzheimer terms found. Using alternative neurodegenerative disease term: \", neuro_terms[1])       a_term <- neuro_terms[1]     } else {       # If still no match, let's use any disease term for demonstration       disease_terms <- grep(\"disease\", rownames(co_matrix), value = TRUE, ignore.case = TRUE)       if (length(disease_terms) > 0) {         message(\"No neurodegenerative terms found. Using general disease term for demonstration: \", disease_terms[1])         a_term <- disease_terms[1]       } else {         # Last resort - take any term marked as disease in the entity types         if (!is.null(attr(co_matrix, \"entity_types\"))) {           disease_entities <- names(attr(co_matrix, \"entity_types\"))[attr(co_matrix, \"entity_types\") == \"disease\"]           if (length(disease_entities) > 0) {             a_term <- disease_entities[1]             message(\"Using first disease entity: \", a_term)           } else {             stop(\"No suitable disease terms found in the co-occurrence matrix\")           }         } else {           stop(\"No terms found in the co-occurrence matrix\")         }       }     }   } } else {   message(\"Found exact match for primary term\") }  # Check matrix dimensions dim(co_matrix) #> [1] 18 18"},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"classic-abc-model","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery","what":"1. Classic ABC Model","title":"Working with Discovery Models","text":"ABC model identifies potential connections term (starting point) term C (potential discoveries) shared B terms.","code":"# Apply the ABC model with lower thresholds abc_results <- abc_model(   co_matrix,   a_term = a_term,   min_score = 0.0001,  # Much lower threshold   n_results = 500  # Get more results ) #> Filtered out 0 B terms that were too similar to A term (similarity threshold: 0.8) #> Identifying potential C terms via 6 B terms... #>   |                                                                              |                                                                      |   0%  |                                                                              |============                                                          |  17%  |                                                                              |=======================                                               |  33%  |                                                                              |===================================                                   |  50%  |                                                                              |===============================================                       |  67%  |                                                                              |==========================================================            |  83%  |                                                                              |======================================================================| 100%  # View top results if (nrow(abc_results) > 0) {   head(abc_results[, c(\"a_term\", \"b_term\", \"c_term\", \"abc_score\")]) } else {   message(\"No results from ABC model\") } #>           a_term    b_term   c_term    abc_score #> fatigue   cancer   fatigue headache 0.0005243055 #> Receptors cancer Receptors migraine 0.0005235824"},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"statistical-validation-of-abc-results","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery > 1. Classic ABC Model","what":"Statistical Validation of ABC Results","title":"Working with Discovery Models","text":"can apply statistical validation evaluate significance discovered connections:","code":"validated_results <- validate_abc(   abc_results,   co_matrix,   alpha = 0.05,   correction = \"BH\" ) #> Using optimized approach for large matrix validation... #> Using metadata for document count: 271 #> Calculating statistical significance using hypergeometric test... #> 0.0% of connections are statistically significant (p < 0.05, BH correction)  # View statistically significant connections significant_results <- validated_results[validated_results$significant, ] if (nrow(significant_results) > 0) {   head(significant_results[, c(\"a_term\", \"b_term\", \"c_term\", \"abc_score\", \"p_value\")]) } else {   message(\"No statistically significant results found\") } #> No statistically significant results found"},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"anc-model","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery","what":"2. AnC Model","title":"Working with Discovery Models","text":"AnC model extends ABC model considering multiple intermediate B terms:","code":"# Apply the AnC model with lower thresholds anc_results <- anc_model(   co_matrix,   a_term = a_term,   n_b_terms = 10,  # Consider more B terms   c_type = NULL,  # Allow all entity types, not just drugs   min_score = 0.0001,  # Lower threshold   n_results = 500  # Get more results ) #> Validating biomedical relevance of B terms... #> Retained 5 biomedically relevant B terms after filtering #> Validating biomedical relevance of C terms... #> Retained 12 biomedically relevant C terms after filtering #> Analyzing 12 potential C terms... #>   |                                                                              |                                                                      |   0%  |                                                                              |======                                                                |   8%  |                                                                              |============                                                          |  17%  |                                                                              |==================                                                    |  25%  |                                                                              |=======================                                               |  33%  |                                                                              |=============================                                         |  42%  |                                                                              |===================================                                   |  50%  |                                                                              |=========================================                             |  58%  |                                                                              |===============================================                       |  67%  |                                                                              |====================================================                  |  75%  |                                                                              |==========================================================            |  83%  |                                                                              |================================================================      |  92%  |                                                                              |======================================================================| 100% #> No AnC connections found  # View top results if (nrow(anc_results) > 0) {   head(anc_results[, c(\"a_term\", \"b_terms\", \"c_term\", \"anc_score\")]) } else {   message(\"No results from AnC model\") } #> No results from AnC model"},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"bitola-model","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery","what":"3. BITOLA Model","title":"Working with Discovery Models","text":"BITOLA model incorporates semantic types guide discovery process:","code":"# Apply the BITOLA model with appropriate semantic types bitola_results <- bitola_model(   co_matrix,   a_term = a_term,   a_semantic_type = \"disease\",    # Set semantic type for source term   c_semantic_type = \"drug\",       # Set semantic type for target terms   min_score = 0.0001,  # Lower threshold   n_results = 500  # Get more results ) #> Analyzing 6 B terms... #>   |                                                                              |                                                                      |   0%  |                                                                              |============                                                          |  17%  |                                                                              |=======================                                               |  33%  |                                                                              |===================================                                   |  50%  |                                                                              |===============================================                       |  67%  |                                                                              |==========================================================            |  83%  |                                                                              |======================================================================| 100% #> No BITOLA connections found  # View top results if (nrow(bitola_results) > 0) {   head(bitola_results[, c(\"a_term\", \"c_term\", \"support\", \"bitola_score\", \"ranking_score\")]) } else {   message(\"No results from BITOLA model\") } #> No results from BITOLA model"},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"lsi-model-latent-semantic-indexing","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery","what":"4. LSI Model (Latent Semantic Indexing)","title":"Working with Discovery Models","text":"LSI model uses dimensionality reduction discover semantically related terms:","code":"# Create a term-document matrix for LSI model tdm <- create_tdm(preprocessed_articles)  # Apply the LSI model lsi_results <- lsi_model(   tdm,   a_term = a_term,   n_factors = 100,  # Number of latent factors   n_results = 500 )  # View top results if (nrow(lsi_results) > 0) {   head(lsi_results[, c(\"a_term\", \"c_term\", \"lsi_similarity\")]) } else {   message(\"No results from LSI model\") } #>   a_term         c_term lsi_similarity #> 1 cancer adenocarcinoma      0.9524903 #> 2 cancer      carcinoma      0.9492129 #> 3 cancer        peptide      0.1920824 #> 4 cancer      secretase      0.1713798 #> 5 cancer        disease      0.1549414 #> 6 cancer           base      0.1500490"},{"path":"https://liu-chao.site/LBDiscover/articles/Work_with_Discovery_Models.html","id":"comparing-results-from-different-models","dir":"Articles","previous_headings":"Discovery Models in Literature-Based Discovery","what":"Comparing Results from Different Models","title":"Working with Discovery Models","text":"can insightful compare results different discovery models:","code":"# Extract C terms from each model abc_c_terms <- if (nrow(abc_results) > 0) unique(abc_results$c_term) else character(0) anc_c_terms <- if (nrow(anc_results) > 0) unique(anc_results$c_term) else character(0) bitola_c_terms <- if (nrow(bitola_results) > 0) unique(bitola_results$c_term) else character(0) lsi_c_terms <- if (nrow(lsi_results) > 0) unique(lsi_results$c_term) else character(0)  # Find common C terms across models common_abc_anc <- intersect(abc_c_terms, anc_c_terms) common_abc_bitola <- intersect(abc_c_terms, bitola_c_terms) common_abc_lsi <- intersect(abc_c_terms, lsi_c_terms) common_all <- Reduce(intersect, list(abc_c_terms, anc_c_terms, bitola_c_terms, lsi_c_terms))  # Print comparison cat(\"Number of unique C terms found by each model:\\n\") #> Number of unique C terms found by each model: cat(\"  ABC model:\", length(abc_c_terms), \"\\n\") #>   ABC model: 2 cat(\"  AnC model:\", length(anc_c_terms), \"\\n\") #>   AnC model: 0 cat(\"  BITOLA model:\", length(bitola_c_terms), \"\\n\") #>   BITOLA model: 0 cat(\"  LSI model:\", length(lsi_c_terms), \"\\n\\n\") #>   LSI model: 185  cat(\"Number of C terms found by multiple models:\\n\") #> Number of C terms found by multiple models: cat(\"  Common between ABC and AnC:\", length(common_abc_anc), \"\\n\") #>   Common between ABC and AnC: 0 cat(\"  Common between ABC and BITOLA:\", length(common_abc_bitola), \"\\n\") #>   Common between ABC and BITOLA: 0 cat(\"  Common between ABC and LSI:\", length(common_abc_lsi), \"\\n\") #>   Common between ABC and LSI: 0 cat(\"  Common across all four models:\", length(common_all), \"\\n\\n\") #>   Common across all four models: 0  if (length(common_all) > 0) {   cat(\"C terms found by all models:\\n\")   print(common_all) }"},{"path":"https://liu-chao.site/LBDiscover/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Chao Liu. Author, maintainer.","code":""},{"path":"https://liu-chao.site/LBDiscover/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chao Liu (2025). LBDiscover: Literature-Based Discovery Tools Biomedical Research. R package version 0.1.0, https://github.com/chaoliu-cl/LBDiscover.","code":"@Manual{,   title = {LBDiscover: Literature-Based Discovery Tools for Biomedical Research},   author = {{Chao Liu}},   year = {2025},   note = {R package version 0.1.0},   url = {https://github.com/chaoliu-cl/LBDiscover}, }"},{"path":[]},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Literature-Based Discovery Tools for Biomedical Research","text":"LBDiscover R package literature-based discovery (LBD) biomedical research. provides comprehensive suite tools retrieving scientific articles, extracting biomedical entities, building co-occurrence networks, applying various discovery models uncover hidden connections scientific literature. package implements several literature-based discovery approaches including: ABC model (Swanson’s discovery model) AnC model (improved version better biomedical term filtering) Latent Semantic Indexing (LSI) BITOLA-style approaches LBDiscover also features powerful visualization tools exploring discovered connections using networks, heatmaps, interactive diagrams.","code":""},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Literature-Based Discovery Tools for Biomedical Research","text":"","code":"# Install from CRAN install.packages(\"LBDiscover\")  # Or install the development version from GitHub # install.packages(\"devtools\") devtools::install_github(\"chaoliu-cl/LBDiscover\")"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"Literature-Based Discovery Tools for Biomedical Research","text":"LBDiscover provides complete workflow literature-based discovery: Data Retrieval: Query retrieve scientific articles PubMed NCBI databases Text Preprocessing: Clean prepare text analysis Entity Extraction: Identify biomedical entities text (diseases, drugs, genes, etc.) Co-occurrence Analysis: Build networks entity co-occurrences Discovery Models: Apply various discovery algorithms find hidden connections Validation: Validate discoveries statistical tests Visualization: Explore results network graphs, heatmaps, ","code":""},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"quick-start-example","dir":"","previous_headings":"","what":"Quick Start Example","title":"Literature-Based Discovery Tools for Biomedical Research","text":"","code":"library(LBDiscover)  # Retrieve articles from PubMed articles <- pubmed_search(\"migraine treatment\", max_results = 100)  # Preprocess article text preprocessed <- vec_preprocess(   articles,   text_column = \"abstract\",   remove_stopwords = TRUE )  # Extract biomedical entities entities <- extract_entities_workflow(   preprocessed,   text_column = \"abstract\",   entity_types = c(\"disease\", \"drug\", \"gene\") )  # Create co-occurrence matrix co_matrix <- create_comat(   entities,   doc_id_col = \"doc_id\",   entity_col = \"entity\",   type_col = \"entity_type\" )  # Apply the ABC model to find new connections abc_results <- abc_model(   co_matrix,   a_term = \"migraine\",   n_results = 50,   scoring_method = \"combined\" )  # Visualize the results vis_abc_network(abc_results, top_n = 20)"},{"path":[]},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"abc-model","dir":"","previous_headings":"Discovery Models","what":"ABC Model","title":"Literature-Based Discovery Tools for Biomedical Research","text":"ABC model based Swanson’s discovery paradigm. concept related concept B, concept B related concept C, C directly connected literature, may hidden relationship C.","code":"# Apply the ABC model abc_results <- abc_model(   co_matrix,   a_term = \"migraine\",   min_score = 0.1,   n_results = 50 )  # Visualize as a network vis_abc_network(abc_results)  # Or as a heatmap vis_heatmap(abc_results)"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"anc-model","dir":"","previous_headings":"Discovery Models","what":"AnC Model","title":"Literature-Based Discovery Tools for Biomedical Research","text":"AnC model extension ABC model uses multiple B terms establish stronger connections C.","code":"# Apply the AnC model anc_results <- anc_model(   co_matrix,   a_term = \"migraine\",   n_b_terms = 5,   min_score = 0.1 )"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"lsi-model","dir":"","previous_headings":"Discovery Models","what":"LSI Model","title":"Literature-Based Discovery Tools for Biomedical Research","text":"Latent Semantic Indexing model identifies semantically related terms using dimensionality reduction techniques.","code":"# Create term-document matrix tdm <- create_term_document_matrix(preprocessed)  # Apply LSI model lsi_results <- lsi_model(   tdm,   a_term = \"migraine\",   n_factors = 100 )"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"visualization","dir":"","previous_headings":"","what":"Visualization","title":"Literature-Based Discovery Tools for Biomedical Research","text":"package offers multiple visualization options:","code":"# Network visualization vis_abc_network(abc_results, top_n = 25)  # Heatmap of connections vis_heatmap(abc_results, top_n = 20)  # Export interactive HTML network export_network(abc_results, output_file = \"abc_network.html\")  # Export interactive chord diagram export_chord(abc_results, output_file = \"abc_chord.html\")"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"comprehensive-analysis","dir":"","previous_headings":"","what":"Comprehensive Analysis","title":"Literature-Based Discovery Tools for Biomedical Research","text":"end--end analysis:","code":"# Run comprehensive discovery analysis discovery_results <- run_lbd(   search_query = \"migraine pathophysiology\",   a_term = \"migraine\",   discovery_approaches = c(\"abc\", \"anc\", \"lsi\"),   include_visualizations = TRUE,   output_file = \"discovery_report.html\" )"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Literature-Based Discovery Tools for Biomedical Research","text":"detailed documentation examples, please see package vignettes:","code":"# View package vignettes browseVignettes(\"LBDiscover\")"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Literature-Based Discovery Tools for Biomedical Research","text":"use LBDiscover research, please cite:","code":"Liu, C. (2025). LBDiscover: Literature-Based Discovery Tools for Biomedical Research.  R package version 0.1.0. https://github.com/chaoliu-cl/LBDiscover"},{"path":"https://liu-chao.site/LBDiscover/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Literature-Based Discovery Tools for Biomedical Research","text":"project licensed GPL-3 License - see LICENSE file details.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply the ABC model for literature-based discovery with improved filtering — abc_model","title":"Apply the ABC model for literature-based discovery with improved filtering — abc_model","text":"function implements ABC model literature-based discovery enhanced term filtering validation.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply the ABC model for literature-based discovery with improved filtering — abc_model","text":"","code":"abc_model(   co_matrix,   a_term,   c_term = NULL,   min_score = 0.1,   n_results = 100,   scoring_method = c(\"multiplication\", \"average\", \"combined\", \"jaccard\"),   b_term_types = NULL,   c_term_types = NULL,   exclude_general_terms = TRUE,   filter_similar_terms = TRUE,   similarity_threshold = 0.8,   enforce_strict_typing = TRUE,   validation_method = \"pattern\" )"},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply the ABC model for literature-based discovery with improved filtering — abc_model","text":"co_matrix co-occurrence matrix produced create_comat(). a_term Character string, source term (). c_term Character string, target term (C). NULL, potential C terms evaluated. min_score Minimum score threshold results. n_results Maximum number results return. scoring_method Method use scoring. b_term_types Character vector entity types allowed B terms. c_term_types Character vector entity types allowed C terms. exclude_general_terms Logical. TRUE, excludes common general terms. filter_similar_terms Logical. TRUE, filters B-terms similar -term. similarity_threshold Numeric. Maximum allowed string similarity B terms. enforce_strict_typing Logical. TRUE, enforces stricter entity type validation. validation_method Character. Method use entity validation: \"pattern\", \"nlp\", \"api\", \"comprehensive\".","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply the ABC model for literature-based discovery with improved filtering — abc_model","text":"data frame ranked discovery results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_opt.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimize ABC model calculations for large matrices — abc_model_opt","title":"Optimize ABC model calculations for large matrices — abc_model_opt","text":"function implements optimized version ABC model calculation efficient large co-occurrence matrices.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_opt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimize ABC model calculations for large matrices — abc_model_opt","text":"","code":"abc_model_opt(   co_matrix,   a_term,   c_term = NULL,   min_score = 0.1,   n_results = 100,   chunk_size = 500 )"},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_opt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimize ABC model calculations for large matrices — abc_model_opt","text":"co_matrix co-occurrence matrix produced create_cooccurrence_matrix(). a_term Character string, source term (). c_term Character string, target term (C). NULL, potential C terms evaluated. min_score Minimum score threshold results. n_results Maximum number results return. chunk_size Number B terms process chunk.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_opt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimize ABC model calculations for large matrices — abc_model_opt","text":"data frame ranked discovery results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_opt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimize ABC model calculations for large matrices — abc_model_opt","text":"","code":"if (FALSE) { # \\dontrun{ abc_results <- abc_model_opt(co_matrix, a_term = \"migraine\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_sig.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply the ABC model with statistical significance testing — abc_model_sig","title":"Apply the ABC model with statistical significance testing — abc_model_sig","text":"function extends ABC model statistical significance testing evaluate strength discovered connections.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_sig.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply the ABC model with statistical significance testing — abc_model_sig","text":"","code":"abc_model_sig(   co_matrix,   a_term,   c_term = NULL,   a_type = NULL,   c_type = NULL,   min_score = 0.1,   n_results = 100,   n_permutations = 1000,   scoring_method = c(\"multiplication\", \"average\", \"combined\", \"jaccard\") )"},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_sig.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply the ABC model with statistical significance testing — abc_model_sig","text":"co_matrix co-occurrence matrix produced create_cooccurrence_matrix(). a_term Character string, source term (). c_term Character string, target term (C). NULL, potential C terms evaluated. a_type Character string, entity type terms. NULL, types considered. c_type Character string, entity type C terms. NULL, types considered. min_score Minimum score threshold results. n_results Maximum number results return. n_permutations Number permutations significance testing. scoring_method Method use scoring ABC connections.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_sig.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply the ABC model with statistical significance testing — abc_model_sig","text":"data frame ranked discovery results p-values.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_model_sig.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply the ABC model with statistical significance testing — abc_model_sig","text":"","code":"if (FALSE) { # \\dontrun{ abc_results <- abc_model_sig(co_matrix, a_term = \"migraine\",                                                 scoring_method = \"combined\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/abc_timeslice.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply time-sliced ABC model for validation — abc_timeslice","title":"Apply time-sliced ABC model for validation — abc_timeslice","text":"function implements time-sliced ABC model validation. uses historical data predict connections appear future.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_timeslice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply time-sliced ABC model for validation — abc_timeslice","text":"","code":"abc_timeslice(   entity_data,   time_column = \"publication_year\",   split_time,   a_term,   a_type = NULL,   c_type = NULL,   min_score = 0.1,   n_results = 100 )"},{"path":"https://liu-chao.site/LBDiscover/reference/abc_timeslice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply time-sliced ABC model for validation — abc_timeslice","text":"entity_data data frame entity data time information. time_column Name column containing time information. split_time Time point split historical future data. a_term Character string, source term (). a_type Character string, entity type terms. c_type Character string, entity type C terms. min_score Minimum score threshold results. n_results Maximum number results return.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_timeslice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply time-sliced ABC model for validation — abc_timeslice","text":"list prediction results validation metrics.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/abc_timeslice.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply time-sliced ABC model for validation — abc_timeslice","text":"","code":"if (FALSE) { # \\dontrun{ validation <- abc_timeslice(entity_data,                             time_column = \"publication_year\",                             split_time = 2010,                             a_term = \"migraine\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/add_statistical_significance.html","id":null,"dir":"Reference","previous_headings":"","what":"Add statistical significance testing based on hypergeometric tests — add_statistical_significance","title":"Add statistical significance testing based on hypergeometric tests — add_statistical_significance","text":"Add statistical significance testing based hypergeometric tests","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/add_statistical_significance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add statistical significance testing based on hypergeometric tests — add_statistical_significance","text":"","code":"add_statistical_significance(results, co_matrix, alpha = 0.05)"},{"path":"https://liu-chao.site/LBDiscover/reference/add_statistical_significance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add statistical significance testing based on hypergeometric tests — add_statistical_significance","text":"results Data frame ABC model results co_matrix Co-occurrence matrix alpha Significance level","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/add_statistical_significance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add statistical significance testing based on hypergeometric tests — add_statistical_significance","text":"Data frame p-values significance indicators","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/alternative_validation.html","id":null,"dir":"Reference","previous_headings":"","what":"Alternative validation for large matrices — alternative_validation","title":"Alternative validation for large matrices — alternative_validation","text":"Alternative validation large matrices","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/alternative_validation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alternative validation for large matrices — alternative_validation","text":"","code":"alternative_validation(abc_results, co_matrix, alpha, correction)"},{"path":"https://liu-chao.site/LBDiscover/reference/anc_model.html","id":null,"dir":"Reference","previous_headings":"","what":"ANC model for literature-based discovery with biomedical term filtering — anc_model","title":"ANC model for literature-based discovery with biomedical term filtering — anc_model","text":"function implements improved ANC model ensures biomedical terms used intermediaries.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/anc_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ANC model for literature-based discovery with biomedical term filtering — anc_model","text":"","code":"anc_model(   co_matrix,   a_term,   n_b_terms = 3,   c_type = NULL,   min_score = 0.1,   n_results = 100,   enforce_biomedical_terms = TRUE,   b_term_types = c(\"protein\", \"gene\", \"chemical\", \"pathway\", \"drug\", \"disease\",     \"biological_process\"),   validation_function = is_valid_biomedical_entity )"},{"path":"https://liu-chao.site/LBDiscover/reference/anc_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ANC model for literature-based discovery with biomedical term filtering — anc_model","text":"co_matrix co-occurrence matrix produced create_cooccurrence_matrix(). a_term Character string, source term (). n_b_terms Number intermediate B terms consider. c_type Character string, entity type C terms. NULL, types considered. min_score Minimum score threshold results. n_results Maximum number results return. enforce_biomedical_terms Logical. TRUE, enforces strict biomedical term filtering. b_term_types Character vector entity types allowed B terms. validation_function Function validate biomedical terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/anc_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ANC model for literature-based discovery with biomedical term filtering — anc_model","text":"data frame ranked discovery results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/apply_bitola_flexible.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply a flexible BITOLA-style discovery model without strict type constraints — apply_bitola_flexible","title":"Apply a flexible BITOLA-style discovery model without strict type constraints — apply_bitola_flexible","text":"function implements modified BITOLA-style discovery model preserves entity type information enforce strict type constraints.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/apply_bitola_flexible.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply a flexible BITOLA-style discovery model without strict type constraints — apply_bitola_flexible","text":"","code":"apply_bitola_flexible(co_matrix, a_term, min_score = 0.1, n_results = 100)"},{"path":"https://liu-chao.site/LBDiscover/reference/apply_bitola_flexible.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply a flexible BITOLA-style discovery model without strict type constraints — apply_bitola_flexible","text":"co_matrix co-occurrence matrix entity types attribute. a_term Character string, source term (). min_score Minimum score threshold results. n_results Maximum number results return.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/apply_bitola_flexible.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply a flexible BITOLA-style discovery model without strict type constraints — apply_bitola_flexible","text":"data frame ranked discovery results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/apply_correction.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply correction to p-values — apply_correction","title":"Apply correction to p-values — apply_correction","text":"Apply correction p-values","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/apply_correction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply correction to p-values — apply_correction","text":"","code":"apply_correction(results, correction, alpha)"},{"path":"https://liu-chao.site/LBDiscover/reference/authenticate_umls.html","id":null,"dir":"Reference","previous_headings":"","what":"Authenticate with UMLS — authenticate_umls","title":"Authenticate with UMLS — authenticate_umls","text":"function authenticates UMLS returns TGT URL.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/authenticate_umls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Authenticate with UMLS — authenticate_umls","text":"","code":"authenticate_umls(api_key)"},{"path":"https://liu-chao.site/LBDiscover/reference/authenticate_umls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Authenticate with UMLS — authenticate_umls","text":"api_key UMLS API key","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/authenticate_umls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Authenticate with UMLS — authenticate_umls","text":"Character string TGT URL NULL authentication fails","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/bitola_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply BITOLA-style discovery model — bitola_model","title":"Apply BITOLA-style discovery model — bitola_model","text":"function implements BITOLA-style discovery model based MeSH term co-occurrence semantic type filtering.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/bitola_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply BITOLA-style discovery model — bitola_model","text":"","code":"bitola_model(   co_matrix,   a_term,   a_semantic_type = NULL,   c_semantic_type = NULL,   min_score = 0.1,   n_results = 100 )"},{"path":"https://liu-chao.site/LBDiscover/reference/bitola_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply BITOLA-style discovery model — bitola_model","text":"co_matrix co-occurrence matrix produced create_cooccurrence_matrix(). a_term Character string, source term (). a_semantic_type Character string, semantic type term. c_semantic_type Character string, semantic type C terms. min_score Minimum score threshold results. n_results Maximum number results return.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/bitola_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply BITOLA-style discovery model — bitola_model","text":"data frame ranked discovery results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/bitola_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply BITOLA-style discovery model — bitola_model","text":"","code":"if (FALSE) { # \\dontrun{ bitola_results <- bitola_model(co_matrix, a_term = \"migraine\",                                    a_semantic_type = \"Disease\",                                    c_semantic_type = \"Gene\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/calc_bibliometrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate basic bibliometric statistics — calc_bibliometrics","title":"Calculate basic bibliometric statistics — calc_bibliometrics","text":"function calculates basic bibliometric statistics article data.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calc_bibliometrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate basic bibliometric statistics — calc_bibliometrics","text":"","code":"calc_bibliometrics(article_data, by_year = TRUE)"},{"path":"https://liu-chao.site/LBDiscover/reference/calc_bibliometrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate basic bibliometric statistics — calc_bibliometrics","text":"article_data data frame containing article data. by_year Logical. TRUE, calculates statistics year.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calc_bibliometrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate basic bibliometric statistics — calc_bibliometrics","text":"list containing bibliometric statistics.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calc_bibliometrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate basic bibliometric statistics — calc_bibliometrics","text":"","code":"if (FALSE) { # \\dontrun{ stats <- calc_bibliometrics(article_data) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/calc_doc_sim.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate document similarity using TF-IDF and cosine similarity — calc_doc_sim","title":"Calculate document similarity using TF-IDF and cosine similarity — calc_doc_sim","text":"function calculates similarity documents using TF-IDF weighting cosine similarity.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calc_doc_sim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate document similarity using TF-IDF and cosine similarity — calc_doc_sim","text":"","code":"calc_doc_sim(   text_data,   text_column = \"abstract\",   min_term_freq = 2,   max_doc_freq = 0.9 )"},{"path":"https://liu-chao.site/LBDiscover/reference/calc_doc_sim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate document similarity using TF-IDF and cosine similarity — calc_doc_sim","text":"text_data data frame containing text data. text_column Name column containing text analyze. min_term_freq Minimum frequency term included. max_doc_freq Maximum document frequency (proportion) term included.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calc_doc_sim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate document similarity using TF-IDF and cosine similarity — calc_doc_sim","text":"similarity matrix documents.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calc_doc_sim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate document similarity using TF-IDF and cosine similarity — calc_doc_sim","text":"","code":"if (FALSE) { # \\dontrun{ sim_matrix <- calc_doc_sim(article_data, text_column = \"abstract\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/calculate_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate ABC score based on specified method — calculate_score","title":"Calculate ABC score based on specified method — calculate_score","text":"Calculate ABC score based specified method","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calculate_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate ABC score based on specified method — calculate_score","text":"","code":"calculate_score(a_b_score, b_c_score, method)"},{"path":"https://liu-chao.site/LBDiscover/reference/calculate_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate ABC score based on specified method — calculate_score","text":"a_b_score -B association score b_c_score B-C association score method Scoring method: \"multiplication\", \"average\", \"combined\", \"jaccard\"","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/calculate_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate ABC score based on specified method — calculate_score","text":"Calculated score","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/clear_pubmed_cache.html","id":null,"dir":"Reference","previous_headings":"","what":"Clear PubMed cache — clear_pubmed_cache","title":"Clear PubMed cache — clear_pubmed_cache","text":"Removes cached PubMed search results","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/clear_pubmed_cache.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clear PubMed cache — clear_pubmed_cache","text":"","code":"clear_pubmed_cache()"},{"path":"https://liu-chao.site/LBDiscover/reference/clear_pubmed_cache.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clear PubMed cache — clear_pubmed_cache","text":"NULL invisibly","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/cluster_docs.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster documents using K-means — cluster_docs","title":"Cluster documents using K-means — cluster_docs","text":"function clusters documents using K-means based TF-IDF vectors.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/cluster_docs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster documents using K-means — cluster_docs","text":"","code":"cluster_docs(   text_data,   text_column = \"abstract\",   n_clusters = 5,   min_term_freq = 2,   max_doc_freq = 0.9,   random_seed = 42 )"},{"path":"https://liu-chao.site/LBDiscover/reference/cluster_docs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster documents using K-means — cluster_docs","text":"text_data data frame containing text data. text_column Name column containing text analyze. n_clusters Number clusters create. min_term_freq Minimum frequency term included. max_doc_freq Maximum document frequency (proportion) term included. random_seed Seed random number generation (reproducibility).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/cluster_docs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cluster documents using K-means — cluster_docs","text":"data frame original data cluster assignments.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/cluster_docs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cluster documents using K-means — cluster_docs","text":"","code":"if (FALSE) { # \\dontrun{ clustered_data <- cluster_docs(article_data, text_column = \"abstract\", n_clusters = 5) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/compare_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare term frequencies between two corpora — compare_terms","title":"Compare term frequencies between two corpora — compare_terms","text":"function compares term frequencies two sets articles.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/compare_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare term frequencies between two corpora — compare_terms","text":"","code":"compare_terms(   corpus1,   corpus2,   text_column = \"abstract\",   corpus1_name = \"Corpus1\",   corpus2_name = \"Corpus2\",   n = 100,   remove_stopwords = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/compare_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare term frequencies between two corpora — compare_terms","text":"corpus1 First corpus (data frame). corpus2 Second corpus (data frame). text_column Name column containing text analyze. corpus1_name Name first corpus output. corpus2_name Name second corpus output. n Number top terms return. remove_stopwords Logical. TRUE, removes stopwords.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/compare_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare term frequencies between two corpora — compare_terms","text":"data frame containing term frequency comparisons.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/compare_terms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare term frequencies between two corpora — compare_terms","text":"","code":"if (FALSE) { # \\dontrun{ comparison <- compare_terms(corpus1, corpus2,                                       corpus1_name = \"Migraine\",                                       corpus2_name = \"Magnesium\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/create_citation_net.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a citation network from article data — create_citation_net","title":"Create a citation network from article data — create_citation_net","text":"function creates citation network article data. Note: Currently placeholder requires citation data available basic PubMed queries.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_citation_net.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a citation network from article data — create_citation_net","text":"","code":"create_citation_net(article_data, citation_data = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/create_citation_net.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a citation network from article data — create_citation_net","text":"article_data data frame containing article data. citation_data data frame containing citation data (optional).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_citation_net.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a citation network from article data — create_citation_net","text":"igraph object representing citation network.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_citation_net.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a citation network from article data — create_citation_net","text":"","code":"if (FALSE) { # \\dontrun{ network <- create_citation_net(article_data) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/create_comat.html","id":null,"dir":"Reference","previous_headings":"","what":"Create co-occurrence matrix without explicit entity type constraints — create_comat","title":"Create co-occurrence matrix without explicit entity type constraints — create_comat","text":"function creates co-occurrence matrix entity data preserving entity type information attribute without enforcing type constraints.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_comat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create co-occurrence matrix without explicit entity type constraints — create_comat","text":"","code":"create_comat(   entity_data,   doc_id_col = \"doc_id\",   entity_col = \"entity\",   count_col = NULL,   type_col = \"entity_type\",   normalize = TRUE,   normalization_method = c(\"cosine\", \"jaccard\", \"dice\") )"},{"path":"https://liu-chao.site/LBDiscover/reference/create_comat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create co-occurrence matrix without explicit entity type constraints — create_comat","text":"entity_data data frame document IDs entities. doc_id_col Name column containing document IDs. entity_col Name column containing entity names. count_col Name column containing entity counts (optional). type_col Name column containing entity types (optional). normalize Logical. TRUE, normalizes co-occurrence matrix. normalization_method Method normalization (\"cosine\", \"jaccard\", \"dice\").","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_comat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create co-occurrence matrix without explicit entity type constraints — create_comat","text":"co-occurrence matrix entity types stored attribute.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_comat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create co-occurrence matrix without explicit entity type constraints — create_comat","text":"","code":"if (FALSE) { # \\dontrun{ co_matrix <- create_comat(entities,                           doc_id_col = \"doc_id\",                           entity_col = \"entity\",                           type_col = \"entity_type\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/create_dummy_dictionary.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to create dummy dictionaries — create_dummy_dictionary","title":"Helper function to create dummy dictionaries — create_dummy_dictionary","text":"Helper function create dummy dictionaries","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_dummy_dictionary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to create dummy dictionaries — create_dummy_dictionary","text":"","code":"create_dummy_dictionary(dictionary_type)"},{"path":"https://liu-chao.site/LBDiscover/reference/create_report.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a comprehensive discovery report — create_report","title":"Generate a comprehensive discovery report — create_report","text":"function generates HTML report summarizing discovery results without enforcing entity type constraints. includes data validation avoid errors publication years data issues.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_report.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a comprehensive discovery report — create_report","text":"","code":"create_report(   results,   visualizations = NULL,   articles = NULL,   output_file = \"discovery_report.html\" )"},{"path":"https://liu-chao.site/LBDiscover/reference/create_report.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a comprehensive discovery report — create_report","text":"results list containing discovery results different approaches. visualizations list containing file paths visualizations. articles data frame containing original articles. output_file File path output HTML report.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_report.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a comprehensive discovery report — create_report","text":"file path created HTML report (invisibly).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_sparse_comat.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a sparse co-occurrence matrix — create_sparse_comat","title":"Create a sparse co-occurrence matrix — create_sparse_comat","text":"function creates sparse co-occurrence matrix entity data, memory-efficient large datasets.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_sparse_comat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a sparse co-occurrence matrix — create_sparse_comat","text":"","code":"create_sparse_comat(   entity_data,   doc_id_col = \"doc_id\",   entity_col = \"entity\",   count_col = NULL,   type_col = NULL,   normalize = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/create_sparse_comat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a sparse co-occurrence matrix — create_sparse_comat","text":"entity_data data frame document IDs entities. doc_id_col Name column containing document IDs. entity_col Name column containing entity names. count_col Name column containing entity counts (optional). type_col Name column containing entity types (optional). normalize Logical. TRUE, normalizes co-occurrence matrix.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_sparse_comat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a sparse co-occurrence matrix — create_sparse_comat","text":"sparse matrix entity co-occurrences.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_sparse_comat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a sparse co-occurrence matrix — create_sparse_comat","text":"","code":"if (FALSE) { # \\dontrun{ co_matrix <- create_sparse_comat(entities,                                               doc_id_col = \"doc_id\",                                               entity_col = \"entity\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/create_tdm.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a term-document matrix from preprocessed text — create_tdm","title":"Create a term-document matrix from preprocessed text — create_tdm","text":"function creates term-document matrix preprocessed text data.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_tdm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a term-document matrix from preprocessed text — create_tdm","text":"","code":"create_tdm(preprocessed_data, min_df = 2, max_df = 0.9)"},{"path":"https://liu-chao.site/LBDiscover/reference/create_tdm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a term-document matrix from preprocessed text — create_tdm","text":"preprocessed_data data frame preprocessed text data. min_df Minimum document frequency term included. max_df Maximum document frequency (proportion) term included.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_tdm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a term-document matrix from preprocessed text — create_tdm","text":"term-document matrix.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_tdm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a term-document matrix from preprocessed text — create_tdm","text":"","code":"if (FALSE) { # \\dontrun{ preprocessed <- preprocess_text(article_data, text_column = \"abstract\") tdm <- create_tdm(preprocessed) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/create_term_document_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a term-document matrix from preprocessed text — create_term_document_matrix","title":"Create a term-document matrix from preprocessed text — create_term_document_matrix","text":"function creates term-document matrix preprocessed text data. simplified version create_tdm() direct use models.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_term_document_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a term-document matrix from preprocessed text — create_term_document_matrix","text":"","code":"create_term_document_matrix(preprocessed_data, min_df = 2, max_df = 0.9)"},{"path":"https://liu-chao.site/LBDiscover/reference/create_term_document_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a term-document matrix from preprocessed text — create_term_document_matrix","text":"preprocessed_data data frame preprocessed text data. min_df Minimum document frequency term included. max_df Maximum document frequency (proportion) term included.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_term_document_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a term-document matrix from preprocessed text — create_term_document_matrix","text":"term-document matrix.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/create_term_document_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a term-document matrix from preprocessed text — create_term_document_matrix","text":"","code":"if (FALSE) { # \\dontrun{ preprocessed <- preprocess_text(article_data, text_column = \"abstract\") tdm <- create_term_document_matrix(preprocessed) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/detect_lang.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect language of text — detect_lang","title":"Detect language of text — detect_lang","text":"function attempts detect language text string. implements simple n-gram based approach require additional packages.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/detect_lang.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect language of text — detect_lang","text":"","code":"detect_lang(text, sample_size = 1000)"},{"path":"https://liu-chao.site/LBDiscover/reference/detect_lang.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect language of text — detect_lang","text":"text Text string analyze sample_size Maximum number characters sample language detection","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/detect_lang.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect language of text — detect_lang","text":"Character string containing ISO 639-1 language code","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/detect_lang.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect language of text — detect_lang","text":"","code":"if (FALSE) { # \\dontrun{ lang <- detect_lang(\"This is English text\") # Returns \"en\" } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_abc.html","id":null,"dir":"Reference","previous_headings":"","what":"Enforce diversity in ABC model results — diversify_abc","title":"Enforce diversity in ABC model results — diversify_abc","text":"function applies diversity enforcement ABC model results : Removing duplicate paths C term Ensuring B term diversity selecting top results B term group Preventing C terms appearing B terms","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_abc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enforce diversity in ABC model results — diversify_abc","text":"","code":"diversify_abc(   abc_results,   diversity_method = c(\"both\", \"b_term_groups\", \"unique_c_paths\"),   max_per_group = 3,   min_score = 0.1 )"},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_abc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enforce diversity in ABC model results — diversify_abc","text":"abc_results data frame containing ABC results. diversity_method Method enforcing diversity: \"b_term_groups\", \"unique_c_paths\", \"\". max_per_group Maximum number results keep per B term C term. min_score Minimum score threshold including connections.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_abc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enforce diversity in ABC model results — diversify_abc","text":"data frame diverse ABC results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_abc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enforce diversity in ABC model results — diversify_abc","text":"","code":"if (FALSE) { # \\dontrun{ diverse_results <- diversify_abc(abc_results) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_b_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Enforce diversity by selecting top connections from each B term — diversify_b_terms","title":"Enforce diversity by selecting top connections from each B term — diversify_b_terms","text":"Enforce diversity selecting top connections B term","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_b_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enforce diversity by selecting top connections from each B term — diversify_b_terms","text":"","code":"diversify_b_terms(results, max_per_group = 3)"},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_b_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enforce diversity by selecting top connections from each B term — diversify_b_terms","text":"results Data frame ABC model results max_per_group Maximum number results keep per B term","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_b_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enforce diversity by selecting top connections from each B term — diversify_b_terms","text":"Data frame diverse results","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_c_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Enforce diversity for C term paths — diversify_c_paths","title":"Enforce diversity for C term paths — diversify_c_paths","text":"Enforce diversity C term paths","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_c_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enforce diversity for C term paths — diversify_c_paths","text":"","code":"diversify_c_paths(results, max_per_c = 3)"},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_c_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enforce diversity for C term paths — diversify_c_paths","text":"results Data frame ABC model results max_per_c Maximum number paths keep per C term","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/diversify_c_paths.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enforce diversity for C term paths — diversify_c_paths","text":"Data frame C term path diversity enforced","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/dot-dict_cache_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Environment to store dictionary cache data — .dict_cache_env","title":"Environment to store dictionary cache data — .dict_cache_env","text":"Environment store dictionary cache data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/dot-dict_cache_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Environment to store dictionary cache data — .dict_cache_env","text":"","code":".dict_cache_env"},{"path":"https://liu-chao.site/LBDiscover/reference/dot-dict_cache_env.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Environment to store dictionary cache data — .dict_cache_env","text":"object class environment length 0.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/dot-pubmed_cache_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Environment to store PubMed cache data — .pubmed_cache_env","title":"Environment to store PubMed cache data — .pubmed_cache_env","text":"Environment store PubMed cache data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/dot-pubmed_cache_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Environment to store PubMed cache data — .pubmed_cache_env","text":"","code":".pubmed_cache_env"},{"path":"https://liu-chao.site/LBDiscover/reference/dot-pubmed_cache_env.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Environment to store PubMed cache data — .pubmed_cache_env","text":"object class environment length 0.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/enhance_abc_kb.html","id":null,"dir":"Reference","previous_headings":"","what":"Enhance ABC results with external knowledge — enhance_abc_kb","title":"Enhance ABC results with external knowledge — enhance_abc_kb","text":"function enhances ABC results information external knowledge bases.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/enhance_abc_kb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enhance ABC results with external knowledge — enhance_abc_kb","text":"","code":"enhance_abc_kb(abc_results, knowledge_base = c(\"umls\", \"mesh\"), api_key = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/enhance_abc_kb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enhance ABC results with external knowledge — enhance_abc_kb","text":"abc_results data frame containing ABC results. knowledge_base Character string, knowledge base use (\"umls\" \"mesh\"). api_key Character string. API key knowledge base (needed).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/enhance_abc_kb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enhance ABC results with external knowledge — enhance_abc_kb","text":"data frame enhanced ABC results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/enhance_abc_kb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enhance ABC results with external knowledge — enhance_abc_kb","text":"","code":"if (FALSE) { # \\dontrun{ enhanced_results <- enhance_abc_kb(abc_results, knowledge_base = \"mesh\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/eval_evidence.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate literature support for discovery results — eval_evidence","title":"Evaluate literature support for discovery results — eval_evidence","text":"function evaluates top results searching supporting evidence literature connections.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/eval_evidence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate literature support for discovery results — eval_evidence","text":"","code":"eval_evidence(   results,   max_results = 5,   base_term = NULL,   max_articles = 5,   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/eval_evidence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate literature support for discovery results — eval_evidence","text":"results results evaluate max_results Maximum number results evaluate (default: 5) base_term base term direct connection queries (e.g., \"migraine\") max_articles Maximum number articles retrieve per search (default: 5) verbose Logical; TRUE, print evaluation results (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/eval_evidence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate literature support for discovery results — eval_evidence","text":"list containing evaluation results","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord.html","id":null,"dir":"Reference","previous_headings":"","what":"Export interactive HTML chord diagram for ABC connections — export_chord","title":"Export interactive HTML chord diagram for ABC connections — export_chord","text":"function creates HTML chord diagram visualization ABC connections.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export interactive HTML chord diagram for ABC connections — export_chord","text":"","code":"export_chord(   abc_results,   output_file = \"abc_chord.html\",   top_n = 50,   min_score = 0.1,   open = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export interactive HTML chord diagram for ABC connections — export_chord","text":"abc_results data frame containing ABC results. output_file File path output HTML file. top_n Number top results visualize. min_score Minimum score threshold including connections. open Logical. TRUE, opens HTML file creation.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export interactive HTML chord diagram for ABC connections — export_chord","text":"file path created HTML file (invisibly).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Export interactive HTML chord diagram for ABC connections — export_chord","text":"","code":"if (FALSE) { # \\dontrun{ export_chord(abc_results, output_file = \"abc_chord.html\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord_diagram.html","id":null,"dir":"Reference","previous_headings":"","what":"Export interactive HTML chord diagram for ABC connections — export_chord_diagram","title":"Export interactive HTML chord diagram for ABC connections — export_chord_diagram","text":"function creates HTML chord diagram visualization ABC connections, properly coloring arcs based whether term , B, C term.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord_diagram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export interactive HTML chord diagram for ABC connections — export_chord_diagram","text":"","code":"export_chord_diagram(   abc_results,   output_file = \"abc_chord.html\",   top_n = 50,   min_score = 0.1,   open = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord_diagram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export interactive HTML chord diagram for ABC connections — export_chord_diagram","text":"abc_results data frame containing ABC results. output_file File path output HTML file. top_n Number top results visualize. min_score Minimum score threshold including connections. open Logical. TRUE, opens HTML file creation.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord_diagram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export interactive HTML chord diagram for ABC connections — export_chord_diagram","text":"file path created HTML file (invisibly).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_chord_diagram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Export interactive HTML chord diagram for ABC connections — export_chord_diagram","text":"","code":"if (FALSE) { # \\dontrun{ export_chord_diagram(abc_results, output_file = \"abc_chord.html\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/export_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Export ABC results to simple HTML network — export_network","title":"Export ABC results to simple HTML network — export_network","text":"function exports ABC results simple HTML file visualization. visNetwork package available, use interactive visualization.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export ABC results to simple HTML network — export_network","text":"","code":"export_network(   abc_results,   output_file = \"abc_network.html\",   top_n = 50,   min_score = 0.1,   open = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/export_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export ABC results to simple HTML network — export_network","text":"abc_results data frame containing ABC results apply_abc_model(). output_file File path output HTML file. top_n Number top results visualize. min_score Minimum score threshold including connections. open Logical. TRUE, opens HTML file creation.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export ABC results to simple HTML network — export_network","text":"file path created HTML file (invisibly).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/export_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Export ABC results to simple HTML network — export_network","text":"","code":"if (FALSE) { # \\dontrun{ export_network(abc_results, output_file = \"abc_network.html\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract and classify entities from text with multi-domain types — extract_entities","title":"Extract and classify entities from text with multi-domain types — extract_entities","text":"function extracts entities text optionally assigns specific semantic categories based dictionaries.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract and classify entities from text with multi-domain types — extract_entities","text":"","code":"extract_entities(   text_data,   text_column = \"abstract\",   dictionary = NULL,   case_sensitive = FALSE,   overlap_strategy = c(\"priority\", \"all\", \"longest\"),   sanitize_dict = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract and classify entities from text with multi-domain types — extract_entities","text":"text_data data frame containing article text data. text_column Name column containing text process. dictionary Combined dictionary list dictionaries entity extraction. case_sensitive Logical. TRUE, matching case-sensitive. overlap_strategy handle terms match multiple dictionaries: \"priority\", \"\", \"longest\". sanitize_dict Logical. TRUE, sanitizes dictionary extraction.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract and classify entities from text with multi-domain types — extract_entities","text":"data frame extracted entities, types, positions.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract entities from text with improved efficiency using only base R — extract_entities_workflow","title":"Extract entities from text with improved efficiency using only base R — extract_entities_workflow","text":"function provides complete workflow extracting entities text using dictionaries multiple sources, improved performance robust error handling.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract entities from text with improved efficiency using only base R — extract_entities_workflow","text":"","code":"extract_entities_workflow(   text_data,   text_column = \"abstract\",   entity_types = c(\"disease\", \"drug\", \"gene\"),   dictionary_sources = c(\"local\", \"mesh\", \"umls\"),   additional_mesh_queries = NULL,   sanitize = TRUE,   api_key = NULL,   custom_dictionary = NULL,   max_terms_per_type = 200,   verbose = TRUE,   batch_size = 500,   parallel = FALSE,   num_cores = 2,   cache_dictionaries = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract entities from text with improved efficiency using only base R — extract_entities_workflow","text":"text_data data frame containing article text data. text_column Name column containing text process. entity_types Character vector entity types include. dictionary_sources Character vector sources entity dictionaries. additional_mesh_queries Named list additional MeSH queries. sanitize Logical. TRUE, sanitizes dictionaries extraction. api_key API key UMLS access (\"umls\" dictionary_sources). custom_dictionary data frame containing custom dictionary entries incorporate entity extraction process. max_terms_per_type Maximum number terms fetch per entity type. Default 200. verbose Logical. TRUE, prints detailed progress information. batch_size Number documents process single batch. Default 500. parallel Logical. TRUE, uses parallel processing available. Default FALSE. num_cores Number cores use parallel processing. Default 2. cache_dictionaries Logical. TRUE, caches dictionaries faster reuse. Default TRUE.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_entities_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract entities from text with improved efficiency using only base R — extract_entities_workflow","text":"data frame extracted entities, types, positions.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_mesh_from_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract MeSH terms from text format instead of XML — extract_mesh_from_text","title":"Extract MeSH terms from text format instead of XML — extract_mesh_from_text","text":"Extract MeSH terms text format instead XML","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_mesh_from_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract MeSH terms from text format instead of XML — extract_mesh_from_text","text":"","code":"extract_mesh_from_text(mesh_text, dictionary_type)"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_mesh_from_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract MeSH terms from text format instead of XML — extract_mesh_from_text","text":"mesh_text Text containing MeSH data non-XML format dictionary_type Type dictionary","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_mesh_from_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract MeSH terms from text format instead of XML — extract_mesh_from_text","text":"data frame MeSH terms","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ner.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform named entity recognition on text — extract_ner","title":"Perform named entity recognition on text — extract_ner","text":"function performs simple dictionary-based named entity recognition. advanced NER, consider using external tools via reticulate.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform named entity recognition on text — extract_ner","text":"","code":"extract_ner(   text,   entity_types = c(\"disease\", \"drug\", \"gene\"),   custom_dictionaries = NULL )"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform named entity recognition on text — extract_ner","text":"text Character vector texts process entity_types Character vector entity types recognize custom_dictionaries List custom dictionaries (named entity type)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform named entity recognition on text — extract_ner","text":"data frame containing found entities, types, positions","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform named entity recognition on text — extract_ner","text":"","code":"if (FALSE) { # \\dontrun{ entities <- extract_ner(abstracts,                                   entity_types = c(\"disease\", \"drug\", \"gene\")) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ngrams.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract n-grams from text — extract_ngrams","title":"Extract n-grams from text — extract_ngrams","text":"function extracts n-grams (sequences n words) text.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ngrams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract n-grams from text — extract_ngrams","text":"","code":"extract_ngrams(text, n = 1, min_freq = 2)"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ngrams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract n-grams from text — extract_ngrams","text":"text Character vector texts process n Integer specifying n-gram size (1 unigrams, 2 bigrams, etc.) min_freq Minimum frequency include n-gram","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ngrams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract n-grams from text — extract_ngrams","text":"data frame containing n-grams frequencies","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_ngrams.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract n-grams from text — extract_ngrams","text":"","code":"if (FALSE) { # \\dontrun{ bigrams <- extract_ngrams(abstracts, n = 2) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract common terms from a corpus — extract_terms","title":"Extract common terms from a corpus — extract_terms","text":"function extracts counts common terms corpus.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract common terms from a corpus — extract_terms","text":"","code":"extract_terms(   article_data,   text_column = \"abstract\",   n = 100,   remove_stopwords = TRUE,   min_word_length = 3 )"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract common terms from a corpus — extract_terms","text":"article_data data frame containing article data. text_column Name column containing text analyze. n Number top terms return. remove_stopwords Logical. TRUE, removes stopwords. min_word_length Minimum word length include.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract common terms from a corpus — extract_terms","text":"data frame containing term counts.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_terms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract common terms from a corpus — extract_terms","text":"","code":"if (FALSE) { # \\dontrun{ common_terms <- extract_terms(article_data, text_column = \"abstract\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply topic modeling to a corpus — extract_topics","title":"Apply topic modeling to a corpus — extract_topics","text":"function implements simple non-negative matrix factorization (NMF) approach topic modeling, without requiring additional packages.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply topic modeling to a corpus — extract_topics","text":"","code":"extract_topics(   text_data,   text_column = \"abstract\",   n_topics = 5,   max_terms = 10,   n_iterations = 50 )"},{"path":"https://liu-chao.site/LBDiscover/reference/extract_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply topic modeling to a corpus — extract_topics","text":"text_data data frame containing text data text_column Name column containing text n_topics Number topics extract max_terms Maximum number terms per topic return n_iterations Number iterations NMF algorithm","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply topic modeling to a corpus — extract_topics","text":"list containing topic-term document-topic matrices","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/extract_topics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply topic modeling to a corpus — extract_topics","text":"","code":"if (FALSE) { # \\dontrun{ topics <- extract_topics(article_data, text_column = \"abstract\", n_topics = 5) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_gene.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch and parse Gene data — fetch_and_parse_gene","title":"Fetch and parse Gene data — fetch_and_parse_gene","text":"Fetch parse Gene data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_gene.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch and parse Gene data — fetch_and_parse_gene","text":"","code":"fetch_and_parse_gene(search_result, max_results, throttle_api, retry_api_call)"},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_pmc.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch and parse PMC data — fetch_and_parse_pmc","title":"Fetch and parse PMC data — fetch_and_parse_pmc","text":"Fetch parse PMC data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_pmc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch and parse PMC data — fetch_and_parse_pmc","text":"","code":"fetch_and_parse_pmc(search_result, max_results, throttle_api, retry_api_call)"},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_protein.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch and parse Protein data — fetch_and_parse_protein","title":"Fetch and parse Protein data — fetch_and_parse_protein","text":"Fetch parse Protein data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_protein.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch and parse Protein data — fetch_and_parse_protein","text":"","code":"fetch_and_parse_protein(   search_result,   max_results,   throttle_api,   retry_api_call )"},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_pubmed.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch and parse PubMed data — fetch_and_parse_pubmed","title":"Fetch and parse PubMed data — fetch_and_parse_pubmed","text":"Fetch parse PubMed data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/fetch_and_parse_pubmed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch and parse PubMed data — fetch_and_parse_pubmed","text":"","code":"fetch_and_parse_pubmed(   search_result,   max_results,   throttle_api,   retry_api_call )"},{"path":"https://liu-chao.site/LBDiscover/reference/filter_by_type.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter a co-occurrence matrix by entity type — filter_by_type","title":"Filter a co-occurrence matrix by entity type — filter_by_type","text":"Filter co-occurrence matrix entity type","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/filter_by_type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter a co-occurrence matrix by entity type — filter_by_type","text":"","code":"filter_by_type(co_matrix, types)"},{"path":"https://liu-chao.site/LBDiscover/reference/filter_by_type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter a co-occurrence matrix by entity type — filter_by_type","text":"co_matrix co-occurrence matrix produced create_typed_comat(). types Character vector entity types include.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/filter_by_type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter a co-occurrence matrix by entity type — filter_by_type","text":"filtered co-occurrence matrix.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/filter_by_type.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter a co-occurrence matrix by entity type — filter_by_type","text":"","code":"if (FALSE) { # \\dontrun{ # Keep only disease and drug entities filtered_matrix <- filter_by_type(co_matrix, types = c(\"disease\", \"drug\")) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/find_abc_all.html","id":null,"dir":"Reference","previous_headings":"","what":"Find all potential ABC connections — find_abc_all","title":"Find all potential ABC connections — find_abc_all","text":"function finds potential ABC connections co-occurrence matrix.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_abc_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find all potential ABC connections — find_abc_all","text":"","code":"find_abc_all(   co_matrix,   a_type = NULL,   c_type = NULL,   min_score = 0.1,   n_results = 1000 )"},{"path":"https://liu-chao.site/LBDiscover/reference/find_abc_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find all potential ABC connections — find_abc_all","text":"co_matrix co-occurrence matrix produced create_comat(). a_type Character string, entity type terms. c_type Character string, entity type C terms. min_score Minimum score threshold results. n_results Maximum number results return.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_abc_all.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find all potential ABC connections — find_abc_all","text":"data frame ranked discovery results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_abc_all.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find all potential ABC connections — find_abc_all","text":"","code":"if (FALSE) { # \\dontrun{ all_abc <- find_abc_all(co_matrix, a_type = \"source\", c_type = \"target\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/find_similar_docs.html","id":null,"dir":"Reference","previous_headings":"","what":"Find similar documents for a given document — find_similar_docs","title":"Find similar documents for a given document — find_similar_docs","text":"function finds documents similar given document based TF-IDF cosine similarity.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_similar_docs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find similar documents for a given document — find_similar_docs","text":"","code":"find_similar_docs(text_data, doc_id, text_column = \"abstract\", n_similar = 5)"},{"path":"https://liu-chao.site/LBDiscover/reference/find_similar_docs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find similar documents for a given document — find_similar_docs","text":"text_data data frame containing text data. doc_id ID document find similar documents . text_column Name column containing text analyze. n_similar Number similar documents return.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_similar_docs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find similar documents for a given document — find_similar_docs","text":"data frame similar documents similarity scores.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_similar_docs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find similar documents for a given document — find_similar_docs","text":"","code":"if (FALSE) { # \\dontrun{ similar_docs <- find_similar_docs(article_data, doc_id = 1,                                      text_column = \"abstract\", n_similar = 5) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/find_term.html","id":null,"dir":"Reference","previous_headings":"","what":"Find primary term in co-occurrence matrix — find_term","title":"Find primary term in co-occurrence matrix — find_term","text":"function verifies primary term exists co-occurrence matrix, , attempts find suitable variation.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_term.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find primary term in co-occurrence matrix — find_term","text":"","code":"find_term(co_matrix, primary_term, verbose = TRUE)"},{"path":"https://liu-chao.site/LBDiscover/reference/find_term.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find primary term in co-occurrence matrix — find_term","text":"co_matrix co-occurrence matrix primary_term primary term find verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/find_term.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find primary term in co-occurrence matrix — find_term","text":"found term (either exact match variation)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/gen_report.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate comprehensive discovery report — gen_report","title":"Generate comprehensive discovery report — gen_report","text":"function creates comprehensive HTML report discovery results visualizations.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/gen_report.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate comprehensive discovery report — gen_report","text":"","code":"gen_report(   results_list,   visualizations = NULL,   articles = NULL,   output_file = \"discoveries.html\",   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/gen_report.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate comprehensive discovery report — gen_report","text":"results_list list result data frames different approaches visualizations list paths visualization files articles Prepared article data output_file Filename output HTML report verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/gen_report.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate comprehensive discovery report — gen_report","text":"Invisible output_file path","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_dict_cache.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dictionary cache environment — get_dict_cache","title":"Get dictionary cache environment — get_dict_cache","text":"Get dictionary cache environment","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_dict_cache.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dictionary cache environment — get_dict_cache","text":"","code":"get_dict_cache()"},{"path":"https://liu-chao.site/LBDiscover/reference/get_dict_cache.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dictionary cache environment — get_dict_cache","text":"environment containing cached dictionary data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_pmc_fulltext.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve full text from PubMed Central — get_pmc_fulltext","title":"Retrieve full text from PubMed Central — get_pmc_fulltext","text":"function retrieves full text articles PubMed Central.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_pmc_fulltext.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve full text from PubMed Central — get_pmc_fulltext","text":"","code":"get_pmc_fulltext(pmids, api_key = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/get_pmc_fulltext.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve full text from PubMed Central — get_pmc_fulltext","text":"pmids Character vector PubMed IDs. api_key Character string. NCBI API key higher rate limits (optional).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_pmc_fulltext.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve full text from PubMed Central — get_pmc_fulltext","text":"data frame containing article metadata full text.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_pmc_fulltext.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve full text from PubMed Central — get_pmc_fulltext","text":"","code":"if (FALSE) { # \\dontrun{ full_texts <- get_pmc_fulltext(c(\"12345678\", \"23456789\")) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/get_pubmed_cache.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the pubmed cache environment — get_pubmed_cache","title":"Get the pubmed cache environment — get_pubmed_cache","text":"Get pubmed cache environment","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_pubmed_cache.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the pubmed cache environment — get_pubmed_cache","text":"","code":"get_pubmed_cache()"},{"path":"https://liu-chao.site/LBDiscover/reference/get_pubmed_cache.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the pubmed cache environment — get_pubmed_cache","text":"environment containing cached PubMed data","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_service_ticket.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a service ticket from a TGT URL — get_service_ticket","title":"Get a service ticket from a TGT URL — get_service_ticket","text":"function gets service ticket specific service using TGT URL.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_service_ticket.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a service ticket from a TGT URL — get_service_ticket","text":"","code":"get_service_ticket(tgt_url)"},{"path":"https://liu-chao.site/LBDiscover/reference/get_service_ticket.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a service ticket from a TGT URL — get_service_ticket","text":"tgt_url Ticket Granting Ticket URL","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_service_ticket.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a service ticket from a TGT URL — get_service_ticket","text":"Character string service ticket NULL fails","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_term_vars.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract term variations from text corpus — get_term_vars","title":"Extract term variations from text corpus — get_term_vars","text":"function identifies variations primary term within corpus articles.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_term_vars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract term variations from text corpus — get_term_vars","text":"","code":"get_term_vars(articles, primary_term, text_col = \"abstract\")"},{"path":"https://liu-chao.site/LBDiscover/reference/get_term_vars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract term variations from text corpus — get_term_vars","text":"articles data frame containing article data text columns primary_term primary term find variations text_col Name column containing text search","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_term_vars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract term variations from text corpus — get_term_vars","text":"character vector unique term variations, sorted length","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_type_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Get entity type distribution from co-occurrence matrix — get_type_dist","title":"Get entity type distribution from co-occurrence matrix — get_type_dist","text":"Get entity type distribution co-occurrence matrix","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_type_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get entity type distribution from co-occurrence matrix — get_type_dist","text":"","code":"get_type_dist(co_matrix)"},{"path":"https://liu-chao.site/LBDiscover/reference/get_type_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get entity type distribution from co-occurrence matrix — get_type_dist","text":"co_matrix co-occurrence matrix produced create_typed_comat().","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_type_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get entity type distribution from co-occurrence matrix — get_type_dist","text":"data frame entity type counts percentages.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_type_dist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get entity type distribution from co-occurrence matrix — get_type_dist","text":"","code":"if (FALSE) { # \\dontrun{ type_dist <- get_type_dist(co_matrix) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/get_umls_semantic_types.html","id":null,"dir":"Reference","previous_headings":"","what":"Get UMLS semantic types for a given dictionary type — get_umls_semantic_types","title":"Get UMLS semantic types for a given dictionary type — get_umls_semantic_types","text":"Helper function map dictionary types UMLS semantic type identifiers","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_umls_semantic_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get UMLS semantic types for a given dictionary type — get_umls_semantic_types","text":"","code":"get_umls_semantic_types(dictionary_type)"},{"path":"https://liu-chao.site/LBDiscover/reference/get_umls_semantic_types.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get UMLS semantic types for a given dictionary type — get_umls_semantic_types","text":"dictionary_type type dictionary get semantic types ","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/get_umls_semantic_types.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get UMLS semantic types for a given dictionary type — get_umls_semantic_types","text":"Vector UMLS semantic type identifiers","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/is_valid_biomedical_entity.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine if a term is likely a specific biomedical entity with improved accuracy — is_valid_biomedical_entity","title":"Determine if a term is likely a specific biomedical entity with improved accuracy — is_valid_biomedical_entity","text":"Determine term likely specific biomedical entity improved accuracy","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/is_valid_biomedical_entity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine if a term is likely a specific biomedical entity with improved accuracy — is_valid_biomedical_entity","text":"","code":"is_valid_biomedical_entity(term, claimed_type = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/is_valid_biomedical_entity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine if a term is likely a specific biomedical entity with improved accuracy — is_valid_biomedical_entity","text":"term Character string, term check claimed_type Character string, claimed entity type term","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/is_valid_biomedical_entity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine if a term is likely a specific biomedical entity with improved accuracy — is_valid_biomedical_entity","text":"Logical, TRUE term likely valid biomedical entity, FALSE otherwise","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/list_to_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a list of articles to a data frame — list_to_df","title":"Convert a list of articles to a data frame — list_to_df","text":"function converts list articles data frame.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/list_to_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a list of articles to a data frame — list_to_df","text":"","code":"list_to_df(articles)"},{"path":"https://liu-chao.site/LBDiscover/reference/list_to_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a list of articles to a data frame — list_to_df","text":"articles list articles, containing metadata.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/list_to_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a list of articles to a data frame — list_to_df","text":"data frame containing article metadata.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_dictionary.html","id":null,"dir":"Reference","previous_headings":"","what":"Load biomedical dictionaries with improved error handling — load_dictionary","title":"Load biomedical dictionaries with improved error handling — load_dictionary","text":"function loads pre-defined biomedical dictionaries fetches terms MeSH/UMLS.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_dictionary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load biomedical dictionaries with improved error handling — load_dictionary","text":"","code":"load_dictionary(   dictionary_type = NULL,   custom_path = NULL,   source = c(\"local\", \"mesh\", \"umls\"),   api_key = NULL,   n_terms = 200,   mesh_query = NULL,   semantic_type_filter = NULL,   sanitize = TRUE,   extended_mesh = FALSE,   mesh_queries = NULL )"},{"path":"https://liu-chao.site/LBDiscover/reference/load_dictionary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load biomedical dictionaries with improved error handling — load_dictionary","text":"dictionary_type Type dictionary load. local dictionaries, limited \"disease\", \"drug\", \"gene\". MeSH UMLS, expanded include semantic categories. custom_path Optional path custom dictionary file. source source fetch terms : \"local\", \"mesh\", \"umls\". api_key UMLS API key authentication (required source = \"umls\"). n_terms Number terms fetch. mesh_query Additional query filter MeSH terms (source = \"mesh\"). semantic_type_filter Filter semantic type (used mainly UMLS). sanitize Logical. TRUE, sanitizes dictionary terms. extended_mesh Logical. TRUE source \"mesh\", uses PubMed search additional terms. mesh_queries Named list MeSH queries different categories (extended_mesh = TRUE).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_dictionary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load biomedical dictionaries with improved error handling — load_dictionary","text":"data frame containing dictionary.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Load terms from MeSH using rentrez with improved error handling — load_from_mesh","title":"Load terms from MeSH using rentrez with improved error handling — load_from_mesh","text":"function uses rentrez package retrieve terms MeSH database.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load terms from MeSH using rentrez with improved error handling — load_from_mesh","text":"","code":"load_from_mesh(dictionary_type, n_terms = 200, query = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load terms from MeSH using rentrez with improved error handling — load_from_mesh","text":"dictionary_type Type dictionary load (e.g., \"disease\", \"drug\", \"gene\"). n_terms Maximum number terms fetch. query Additional query filter MeSH terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_mesh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load terms from MeSH using rentrez with improved error handling — load_from_mesh","text":"data frame containing MeSH terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_umls.html","id":null,"dir":"Reference","previous_headings":"","what":"Load terms from UMLS API — load_from_umls","title":"Load terms from UMLS API — load_from_umls","text":"function retrieves terms UMLS using REST API.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_umls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load terms from UMLS API — load_from_umls","text":"","code":"load_from_umls(dictionary_type, api_key, n_terms = 200, semantic_types = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_umls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load terms from UMLS API — load_from_umls","text":"dictionary_type Type dictionary load (e.g., \"disease\", \"drug\", \"gene\"). api_key UMLS API key authentication. n_terms Maximum number terms fetch. semantic_types Vector semantic type identifiers filter .","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_from_umls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load terms from UMLS API — load_from_umls","text":"data frame containing UMLS terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_mesh_terms_from_pubmed.html","id":null,"dir":"Reference","previous_headings":"","what":"Load terms from MeSH using PubMed search — load_mesh_terms_from_pubmed","title":"Load terms from MeSH using PubMed search — load_mesh_terms_from_pubmed","text":"function enhances MeSH dictionary extracting additional terms PubMed search results using MeSH queries.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_mesh_terms_from_pubmed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load terms from MeSH using PubMed search — load_mesh_terms_from_pubmed","text":"","code":"load_mesh_terms_from_pubmed(   mesh_queries,   max_results = 50,   min_term_length = 3,   sanitize = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/load_mesh_terms_from_pubmed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load terms from MeSH using PubMed search — load_mesh_terms_from_pubmed","text":"mesh_queries named list MeSH queries different categories. max_results Maximum number results retrieve per query. min_term_length Minimum length terms include. sanitize Logical. TRUE, sanitizes extracted terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_mesh_terms_from_pubmed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load terms from MeSH using PubMed search — load_mesh_terms_from_pubmed","text":"data frame containing combined dictionary extracted terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Load saved results from a file — load_results","title":"Load saved results from a file — load_results","text":"function loads previously saved results file.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load saved results from a file — load_results","text":"","code":"load_results(file_path)"},{"path":"https://liu-chao.site/LBDiscover/reference/load_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load saved results from a file — load_results","text":"file_path File path load results .","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load saved results from a file — load_results","text":"data frame containing loaded results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/load_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load saved results from a file — load_results","text":"","code":"if (FALSE) { # \\dontrun{ results <- load_results(\"search_results.csv\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/lsi_model.html","id":null,"dir":"Reference","previous_headings":"","what":"LSI model with enhanced biomedical term filtering and NLP verification — lsi_model","title":"LSI model with enhanced biomedical term filtering and NLP verification — lsi_model","text":"function implements improved LSI model rigorously filters non-biomedical terms results ensure clinical relevance. adds NLP-based validation additional layer filtering.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/lsi_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LSI model with enhanced biomedical term filtering and NLP verification — lsi_model","text":"","code":"lsi_model(   term_doc_matrix,   a_term,   n_factors = 100,   n_results = 100,   enforce_biomedical_terms = TRUE,   c_term_types = NULL,   entity_types = NULL,   validation_function = is_valid_biomedical_entity,   min_word_length = 3,   use_nlp = TRUE,   nlp_threshold = 0.7 )"},{"path":"https://liu-chao.site/LBDiscover/reference/lsi_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LSI model with enhanced biomedical term filtering and NLP verification — lsi_model","text":"term_doc_matrix term-document matrix. a_term Character string, source term (). n_factors Number factors use LSI. n_results Maximum number results return. enforce_biomedical_terms Logical. TRUE, enforces strict biomedical term filtering. c_term_types Character vector entity types allowed C terms. entity_types Named vector entity types (NULL, try detect). validation_function Function validate biomedical terms. min_word_length Minimum word length include. use_nlp Logical. TRUE, uses NLP-based validation biomedical terms. nlp_threshold Numeric 0 1. Minimum confidence NLP validation.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/lsi_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"LSI model with enhanced biomedical term filtering and NLP verification — lsi_model","text":"data frame ranked discovery results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/map_ontology.html","id":null,"dir":"Reference","previous_headings":"","what":"Map terms to biomedical ontologies — map_ontology","title":"Map terms to biomedical ontologies — map_ontology","text":"function maps terms standard biomedical ontologies like MeSH UMLS.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/map_ontology.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map terms to biomedical ontologies — map_ontology","text":"","code":"map_ontology(   terms,   ontology = c(\"mesh\", \"umls\"),   api_key = NULL,   fuzzy_match = FALSE,   similarity_threshold = 0.8,   mesh_query = NULL,   semantic_types = NULL,   dictionary_type = \"disease\" )"},{"path":"https://liu-chao.site/LBDiscover/reference/map_ontology.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map terms to biomedical ontologies — map_ontology","text":"terms Character vector terms map ontology Character string. ontology use: \"mesh\" \"umls\" api_key UMLS API key (required ontology = \"umls\") fuzzy_match Logical. TRUE, allows fuzzy matching terms similarity_threshold Numeric 0 1. Minimum similarity fuzzy matching mesh_query Additional query filter MeSH terms (ontology = \"mesh\") semantic_types Vector semantic types filter UMLS results dictionary_type Type dictionary use (\"disease\", \"drug\", \"gene\", etc.)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/map_ontology.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map terms to biomedical ontologies — map_ontology","text":"data frame mapped terms ontology identifiers","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/map_ontology.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Map terms to biomedical ontologies — map_ontology","text":"","code":"if (FALSE) { # \\dontrun{ # Map terms to MeSH mesh_mappings <- map_ontology(c(\"headache\", \"migraine\"), ontology = \"mesh\")  # Map terms to UMLS with API key umls_mappings <- map_ontology(c(\"headache\", \"migraine\"), ontology = \"umls\",                               api_key = \"your_api_key\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/merge_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine and deduplicate entity datasets — merge_entities","title":"Combine and deduplicate entity datasets — merge_entities","text":"function combines custom standard entity datasets, handling case one might empty, removes duplicates.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/merge_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine and deduplicate entity datasets — merge_entities","text":"","code":"merge_entities(   custom_entities,   standard_entities,   primary_term,   primary_type = \"disease\",   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/merge_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine and deduplicate entity datasets — merge_entities","text":"custom_entities Data frame custom entities (can NULL) standard_entities Data frame standard entities (can NULL) primary_term primary term interest primary_type entity type primary term (default: \"disease\") verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/merge_entities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine and deduplicate entity datasets — merge_entities","text":"data frame combined entities","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/merge_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge multiple search results — merge_results","title":"Merge multiple search results — merge_results","text":"function merges multiple search results single data frame.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/merge_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge multiple search results — merge_results","text":"","code":"merge_results(..., remove_duplicates = TRUE)"},{"path":"https://liu-chao.site/LBDiscover/reference/merge_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge multiple search results — merge_results","text":"... Data frames containing search results. remove_duplicates Logical. TRUE, removes duplicate articles.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/merge_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge multiple search results — merge_results","text":"merged data frame.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/merge_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge multiple search results — merge_results","text":"","code":"if (FALSE) { # \\dontrun{ merged_results <- merge_results(results1, results2, results3) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/min_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Ensure minimum results for visualization — min_results","title":"Ensure minimum results for visualization — min_results","text":"function ensures sufficient results visualization, creating placeholder data necessary.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/min_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ensure minimum results for visualization — min_results","text":"","code":"min_results(   diverse_results,   top_results,   a_term,   min_results = 3,   fallback_count = 15,   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/min_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ensure minimum results for visualization — min_results","text":"diverse_results Current diversified results top_results Original top results a_term primary term analysis min_results Minimum number desired results (default: 3) fallback_count Number top results use fallback (default: 15) verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/min_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ensure minimum results for visualization — min_results","text":"data frame sufficient results visualization","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/ncbi_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Search NCBI databases for articles or data — ncbi_search","title":"Search NCBI databases for articles or data — ncbi_search","text":"function searches various NCBI databases using E-utilities API via rentrez package.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/ncbi_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search NCBI databases for articles or data — ncbi_search","text":"","code":"ncbi_search(   query,   database = \"pubmed\",   max_results = 1000,   use_mesh = FALSE,   date_range = NULL,   api_key = NULL,   retry_count = 3,   retry_delay = 2 )"},{"path":"https://liu-chao.site/LBDiscover/reference/ncbi_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search NCBI databases for articles or data — ncbi_search","text":"query Character string containing search query. database Character string. NCBI database search (e.g., \"pubmed\", \"pmc\", \"gene\", \"protein\"). max_results Maximum number results return. use_mesh Logical. TRUE, attempt map query terms MeSH terms (PubMed ). date_range Character vector length 2 start end dates format \"YYYY/MM/DD\". api_key Character string. NCBI API key higher rate limits (optional). retry_count Integer. Number times retry failed requests. retry_delay Integer. Delay retries seconds.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/ncbi_search.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search NCBI databases for articles or data — ncbi_search","text":"data frame containing search results IDs, titles, metadata.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/ncbi_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search NCBI databases for articles or data — ncbi_search","text":"","code":"if (FALSE) { # \\dontrun{ results <- ncbi_search(\"migraine headache\", database = \"pubmed\", max_results = 100) gene_results <- ncbi_search(\"BRCA1\", database = \"gene\", max_results = 10) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/null_coalesce.html","id":null,"dir":"Reference","previous_headings":"","what":"Null coalescing operator — null_coalesce","title":"Null coalescing operator — null_coalesce","text":"Returns first argument NULL empty, otherwise returns second argument.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/null_coalesce.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Null coalescing operator — null_coalesce","text":"","code":"x %||% y"},{"path":"https://liu-chao.site/LBDiscover/reference/null_coalesce.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Null coalescing operator — null_coalesce","text":"x object test NULL empty y object return x NULL empty","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/null_coalesce.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Null coalescing operator — null_coalesce","text":"Returns x x NULL, empty, missing XML node, otherwise returns y.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/null_coalesce.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Null coalescing operator — null_coalesce","text":"","code":"NULL %||% \"default\"  # returns \"default\" #> [1] \"default\" \"value\" %||% \"default\"  # returns \"value\" #> [1] \"value\""},{"path":"https://liu-chao.site/LBDiscover/reference/parallel_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply parallel processing for document analysis — parallel_analysis","title":"Apply parallel processing for document analysis — parallel_analysis","text":"function uses parallel processing analyze documents faster.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/parallel_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply parallel processing for document analysis — parallel_analysis","text":"","code":"parallel_analysis(   text_data,   analysis_function,   text_column = \"abstract\",   ...,   n_cores = NULL )"},{"path":"https://liu-chao.site/LBDiscover/reference/parallel_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply parallel processing for document analysis — parallel_analysis","text":"text_data data frame containing text data. analysis_function Function apply document. text_column Name column containing text analyze. ... Additional arguments passed analysis function. n_cores Number cores use parallel processing. NULL, uses available cores minus 1.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/parallel_analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply parallel processing for document analysis — parallel_analysis","text":"data frame analysis results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/parallel_analysis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply parallel processing for document analysis — parallel_analysis","text":"","code":"if (FALSE) { # \\dontrun{ # Define a simple analysis function count_words <- function(text) {   words <- unlist(strsplit(tolower(text), \"\\\\s+\"))   return(length(words)) }  # Apply parallel processing results <- parallel_analysis(article_data, count_words, text_column = \"abstract\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/parse_pubmed_xml.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse PubMed XML data with optimized memory usage — parse_pubmed_xml","title":"Parse PubMed XML data with optimized memory usage — parse_pubmed_xml","text":"function parses PubMed XML data data frame using streaming handle large result sets efficiently.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/parse_pubmed_xml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse PubMed XML data with optimized memory usage — parse_pubmed_xml","text":"","code":"parse_pubmed_xml(xml_data, verbose = FALSE)"},{"path":"https://liu-chao.site/LBDiscover/reference/parse_pubmed_xml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse PubMed XML data with optimized memory usage — parse_pubmed_xml","text":"xml_data XML data PubMed. verbose Logical. TRUE, prints progress information.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/parse_pubmed_xml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse PubMed XML data with optimized memory usage — parse_pubmed_xml","text":"data frame containing article metadata.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/perm_test_abc.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform randomization test for ABC model — perm_test_abc","title":"Perform randomization test for ABC model — perm_test_abc","text":"function assesses significance ABC model results randomization. generates null distribution permuting co-occurrence matrix.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/perm_test_abc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform randomization test for ABC model — perm_test_abc","text":"","code":"perm_test_abc(abc_results, co_matrix, n_permutations = 1000, alpha = 0.05)"},{"path":"https://liu-chao.site/LBDiscover/reference/perm_test_abc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform randomization test for ABC model — perm_test_abc","text":"abc_results data frame containing ABC results. co_matrix co-occurrence matrix used generate ABC results. n_permutations Number permutations perform. alpha Significance level.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/perm_test_abc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform randomization test for ABC model — perm_test_abc","text":"data frame ABC results permutation-based significance measures.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/perm_test_abc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform randomization test for ABC model — perm_test_abc","text":"","code":"if (FALSE) { # \\dontrun{ randomized_results <- perm_test_abc(abc_results, co_matrix, n_permutations = 1000) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/plot_heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Create heatmap visualization from results — plot_heatmap","title":"Create heatmap visualization from results — plot_heatmap","text":"function creates heatmap visualization ABC results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/plot_heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create heatmap visualization from results — plot_heatmap","text":"","code":"plot_heatmap(   results,   output_file = \"heatmap.png\",   width = 1200,   height = 900,   resolution = 120,   top_n = 15,   min_score = 1e-04,   color_palette = \"blues\",   show_entity_types = TRUE,   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/plot_heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create heatmap visualization from results — plot_heatmap","text":"results results visualize output_file Filename output PNG (default: \"heatmap.png\") width Width output image (default: 1200) height Height output image (default: 900) resolution Resolution output image (default: 120) top_n Maximum number results include (default: 15) min_score Minimum score threshold (default: 0.0001) color_palette Color palette heatmap (default: \"blues\") show_entity_types Logical; TRUE, show entity types (default: TRUE) verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/plot_heatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create heatmap visualization from results — plot_heatmap","text":"Invisible NULL (creates file side effect)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/plot_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Create network visualization from results — plot_network","title":"Create network visualization from results — plot_network","text":"function creates network visualization ABC results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/plot_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create network visualization from results — plot_network","text":"","code":"plot_network(   results,   output_file = \"network.png\",   width = 1200,   height = 900,   resolution = 120,   top_n = 15,   min_score = 1e-04,   node_size_factor = 5,   color_by = \"type\",   title = \"Network Visualization\",   show_entity_types = TRUE,   label_size = 1,   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/plot_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create network visualization from results — plot_network","text":"results results visualize output_file Filename output PNG (default: \"network.png\") width Width output image (default: 1200) height Height output image (default: 900) resolution Resolution output image (default: 120) top_n Maximum number results include (default: 15) min_score Minimum score threshold (default: 0.0001) node_size_factor Factor scaling node sizes (default: 5) color_by Column use node colors (default: \"type\") title Plot title (default: \"Network Visualization\") show_entity_types Logical; TRUE, show entity types (default: TRUE) label_size Relative size labels (default: 1.0) verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/plot_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create network visualization from results — plot_network","text":"Invisible NULL (creates file side effect)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/prep_articles.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare articles for report generation — prep_articles","title":"Prepare articles for report generation — prep_articles","text":"function ensures article data valid report generation, particularly handling publication years.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/prep_articles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare articles for report generation — prep_articles","text":"","code":"prep_articles(articles, verbose = TRUE)"},{"path":"https://liu-chao.site/LBDiscover/reference/prep_articles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare articles for report generation — prep_articles","text":"articles article data frame (can NULL) verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/prep_articles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare articles for report generation — prep_articles","text":"data frame articles validated publication years","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/preprocess_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess article text — preprocess_text","title":"Preprocess article text — preprocess_text","text":"function preprocesses article text analysis.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/preprocess_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess article text — preprocess_text","text":"","code":"preprocess_text(   text_data,   text_column = \"abstract\",   remove_stopwords = TRUE,   custom_stopwords = NULL,   stem_words = FALSE,   min_word_length = 3,   max_word_length = 50 )"},{"path":"https://liu-chao.site/LBDiscover/reference/preprocess_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess article text — preprocess_text","text":"text_data data frame containing article text data (title, abstract, etc.). text_column Name column containing text process. remove_stopwords Logical. TRUE, removes stopwords. custom_stopwords Character vector additional stopwords remove. stem_words Logical. TRUE, applies stemming words. min_word_length Minimum word length keep. max_word_length Maximum word length keep.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/preprocess_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess article text — preprocess_text","text":"data frame processed text extracted terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/preprocess_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess article text — preprocess_text","text":"","code":"if (FALSE) { # \\dontrun{ processed_data <- preprocess_text(article_data, text_column = \"abstract\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Process MeSH data in chunks to avoid memory issues — process_mesh_chunks","title":"Process MeSH data in chunks to avoid memory issues — process_mesh_chunks","text":"Process MeSH data chunks avoid memory issues","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process MeSH data in chunks to avoid memory issues — process_mesh_chunks","text":"","code":"process_mesh_chunks(mesh_records, dictionary_type)"},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process MeSH data in chunks to avoid memory issues — process_mesh_chunks","text":"mesh_records Large MeSH records data dictionary_type Type dictionary","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process MeSH data in chunks to avoid memory issues — process_mesh_chunks","text":"data frame MeSH terms","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_xml.html","id":null,"dir":"Reference","previous_headings":"","what":"Process MeSH XML data with improved error handling — process_mesh_xml","title":"Process MeSH XML data with improved error handling — process_mesh_xml","text":"Helper function process MeSH XML data extract terms","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_xml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process MeSH XML data with improved error handling — process_mesh_xml","text":"","code":"process_mesh_xml(mesh_records, dictionary_type)"},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_xml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process MeSH XML data with improved error handling — process_mesh_xml","text":"mesh_records XML data MeSH database dictionary_type Type dictionary","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/process_mesh_xml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process MeSH XML data with improved error handling — process_mesh_xml","text":"data frame MeSH terms","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/pubmed_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Search PubMed for articles with optimized performance — pubmed_search","title":"Search PubMed for articles with optimized performance — pubmed_search","text":"function searches PubMed using NCBI E-utilities API via rentrez package. implementation includes optimizations speed, memory efficiency, reliability.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/pubmed_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search PubMed for articles with optimized performance — pubmed_search","text":"","code":"pubmed_search(   query,   max_results = 1000,   use_mesh = FALSE,   date_range = NULL,   api_key = NULL,   batch_size = 200,   verbose = TRUE,   use_cache = TRUE,   retry_count = 3,   retry_delay = 1 )"},{"path":"https://liu-chao.site/LBDiscover/reference/pubmed_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search PubMed for articles with optimized performance — pubmed_search","text":"query Character string containing search query. max_results Maximum number results return. use_mesh Logical. TRUE, attempt map query terms MeSH terms. date_range Character vector length 2 start end dates format \"YYYY/MM/DD\". api_key Character string. NCBI API key higher rate limits (optional). batch_size Integer. Number records fetch batch (default: 200). verbose Logical. TRUE, prints progress information. use_cache Logical. TRUE, cache results avoid redundant API calls. retry_count Integer. Number times retry failed API calls. retry_delay Integer. Initial delay retries seconds.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/pubmed_search.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search PubMed for articles with optimized performance — pubmed_search","text":"data frame containing search results PubMed IDs, titles, metadata.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/pubmed_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search PubMed for articles with optimized performance — pubmed_search","text":"","code":"if (FALSE) { # \\dontrun{ results <- pubmed_search(\"migraine headache\", max_results = 100) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/query_external_api.html","id":null,"dir":"Reference","previous_headings":"","what":"Query external biomedical APIs to validate entity types — query_external_api","title":"Query external biomedical APIs to validate entity types — query_external_api","text":"Query external biomedical APIs validate entity types","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_external_api.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query external biomedical APIs to validate entity types — query_external_api","text":"","code":"query_external_api(term, claimed_type)"},{"path":"https://liu-chao.site/LBDiscover/reference/query_external_api.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query external biomedical APIs to validate entity types — query_external_api","text":"term Character string, term validate claimed_type Character string, claimed entity type","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_external_api.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query external biomedical APIs to validate entity types — query_external_api","text":"Logical indicating term found appropriate database","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_mesh.html","id":null,"dir":"Reference","previous_headings":"","what":"Query for MeSH terms using E-utilities — query_mesh","title":"Query for MeSH terms using E-utilities — query_mesh","text":"Query MeSH terms using E-utilities","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_mesh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query for MeSH terms using E-utilities — query_mesh","text":"","code":"query_mesh(term, api_key = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/query_mesh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query for MeSH terms using E-utilities — query_mesh","text":"term Character string, term query. api_key Character string. NCBI API key (optional).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_mesh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query for MeSH terms using E-utilities — query_mesh","text":"data frame MeSH information term.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_umls.html","id":null,"dir":"Reference","previous_headings":"","what":"Query UMLS for term information — query_umls","title":"Query UMLS for term information — query_umls","text":"Query UMLS term information","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_umls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query UMLS for term information — query_umls","text":"","code":"query_umls(term, api_key, version = \"current\")"},{"path":"https://liu-chao.site/LBDiscover/reference/query_umls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query UMLS for term information — query_umls","text":"term Character string, term query. api_key Character string. UMLS API key. version Character string. UMLS version use.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/query_umls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query UMLS for term information — query_umls","text":"data frame UMLS information term.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/remove_ac_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove A and C terms that appear as B terms — remove_ac_terms","title":"Remove A and C terms that appear as B terms — remove_ac_terms","text":"Remove C terms appear B terms","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/remove_ac_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove A and C terms that appear as B terms — remove_ac_terms","text":"","code":"remove_ac_terms(results)"},{"path":"https://liu-chao.site/LBDiscover/reference/remove_ac_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove A and C terms that appear as B terms — remove_ac_terms","text":"results Data frame ABC model results","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/remove_ac_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove A and C terms that appear as B terms — remove_ac_terms","text":"Data frame C terms removed B terms","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/retry_api_call.html","id":null,"dir":"Reference","previous_headings":"","what":"Retry an API call with exponential backoff — retry_api_call","title":"Retry an API call with exponential backoff — retry_api_call","text":"function retries failed API call exponential backoff.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/retry_api_call.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retry an API call with exponential backoff — retry_api_call","text":"","code":"retry_api_call(fun, ..., verbose = FALSE, retry_count = 3, retry_delay = 1)"},{"path":"https://liu-chao.site/LBDiscover/reference/retry_api_call.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retry an API call with exponential backoff — retry_api_call","text":"fun Function call ... Arguments pass function verbose Logical. TRUE, prints progress information retry_count Integer. Number times retry retry_delay Integer. Initial delay retries seconds","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/retry_api_call.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retry an API call with exponential backoff — retry_api_call","text":"Result function call NULL retries fail","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/run_lbd.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform comprehensive literature-based discovery without type constraints — run_lbd","title":"Perform comprehensive literature-based discovery without type constraints — run_lbd","text":"function performs comprehensive literature-based discovery analysis using multiple approaches without enforcing entity type constraints.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/run_lbd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform comprehensive literature-based discovery without type constraints — run_lbd","text":"","code":"run_lbd(   search_query,   a_term,   max_results = 100,   discovery_approaches = c(\"abc\", \"anc\", \"lsi\", \"bitola\"),   include_visualizations = TRUE,   output_file = \"discovery_report.html\",   api_key = NULL,   dictionary_sources = c(\"local\", \"mesh\", \"umls\"),   entity_categories = c(\"disease\", \"drug\", \"gene\") )"},{"path":"https://liu-chao.site/LBDiscover/reference/run_lbd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform comprehensive literature-based discovery without type constraints — run_lbd","text":"search_query Character string, search query retrieving initial articles. a_term Character string, source term () discovery. max_results Maximum number results return approach. discovery_approaches Character vector, discovery approaches use. include_visualizations Logical. TRUE, generates visualizations. output_file File path output report. api_key Character string. API key PubMed services. dictionary_sources Character vector. Sources entity dictionaries: \"local\", \"mesh\", \"umls\". entity_categories Character vector. Entity categories include.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/run_lbd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform comprehensive literature-based discovery without type constraints — run_lbd","text":"list containing discovery results approaches.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/run_lbd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform comprehensive literature-based discovery without type constraints — run_lbd","text":"","code":"if (FALSE) { # \\dontrun{ discovery_results <- run_lbd(   search_query = \"migraine headache\",   a_term = \"migraine\",   discovery_approaches = c(\"abc\", \"anc\", \"lsi\"),   dictionary_sources = c(\"mesh\"),   entity_categories = c(\"disease\", \"drug\", \"gene\", \"protein\") ) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/safe_diversify.html","id":null,"dir":"Reference","previous_headings":"","what":"Diversify ABC results with error handling — safe_diversify","title":"Diversify ABC results with error handling — safe_diversify","text":"function diversifies ABC results avoid redundancy, error handling ensure results always returned.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/safe_diversify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diversify ABC results with error handling — safe_diversify","text":"","code":"safe_diversify(   top_results,   diversity_method = \"both\",   max_per_group = 5,   min_score = 1e-04,   min_results = 5,   fallback_count = 15,   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/safe_diversify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diversify ABC results with error handling — safe_diversify","text":"top_results top ABC results diversify diversity_method Method diversification (default: \"\") max_per_group Maximum results per group (default: 5) min_score Minimum score threshold (default: 0.0001) min_results Minimum number desired results (default: 5) fallback_count Number top results use diversification fails (default: 15) verbose Logical; TRUE, print status messages (default: TRUE)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/safe_diversify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Diversify ABC results with error handling — safe_diversify","text":"data frame diversified results","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/sanitize_dictionary.html","id":null,"dir":"Reference","previous_headings":"","what":"Enhanced sanitize dictionary function — sanitize_dictionary","title":"Enhanced sanitize dictionary function — sanitize_dictionary","text":"function sanitizes dictionary terms ensure valid entity extraction.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/sanitize_dictionary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enhanced sanitize dictionary function — sanitize_dictionary","text":"","code":"sanitize_dictionary(   dictionary,   term_column = \"term\",   type_column = \"type\",   validate_types = TRUE,   verbose = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/sanitize_dictionary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enhanced sanitize dictionary function — sanitize_dictionary","text":"dictionary data frame containing dictionary terms. term_column name column containing terms sanitize. type_column name column containing entity types. validate_types Logical. TRUE, validates terms claimed type. verbose Logical. TRUE, prints information filtering process.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/sanitize_dictionary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enhanced sanitize dictionary function — sanitize_dictionary","text":"data frame sanitized terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/save_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Save search results to a file — save_results","title":"Save search results to a file — save_results","text":"function saves search results file.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/save_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save search results to a file — save_results","text":"","code":"save_results(results, file_path, format = c(\"csv\", \"rds\", \"xlsx\"))"},{"path":"https://liu-chao.site/LBDiscover/reference/save_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save search results to a file — save_results","text":"results data frame containing search results. file_path File path save results. format File format use. One \"csv\", \"rds\", \"xlsx\".","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/save_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save search results to a file — save_results","text":"file path (invisibly).","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/save_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save search results to a file — save_results","text":"","code":"if (FALSE) { # \\dontrun{ save_results(search_results, file_path = \"search_results.csv\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/segment_sentences.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform sentence segmentation on text — segment_sentences","title":"Perform sentence segmentation on text — segment_sentences","text":"function splits text sentences.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/segment_sentences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform sentence segmentation on text — segment_sentences","text":"","code":"segment_sentences(text)"},{"path":"https://liu-chao.site/LBDiscover/reference/segment_sentences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform sentence segmentation on text — segment_sentences","text":"text Character vector texts process","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/segment_sentences.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform sentence segmentation on text — segment_sentences","text":"list element contains character vector sentences","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/segment_sentences.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform sentence segmentation on text — segment_sentences","text":"","code":"if (FALSE) { # \\dontrun{ sentences <- segment_sentences(abstracts) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/shadowtext.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to draw text with a shadow/background — shadowtext","title":"Helper function to draw text with a shadow/background — shadowtext","text":"Helper function draw text shadow/background Helper function draw text shadow/background","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/shadowtext.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to draw text with a shadow/background — shadowtext","text":"","code":"shadowtext(   x,   y,   labels,   col = \"black\",   bg = \"white\",   pos = NULL,   offset = 0.5,   cex = 1,   ... )  shadowtext(   x,   y,   labels,   col = \"black\",   bg = \"white\",   pos = NULL,   offset = 0.5,   cex = 1,   ... )"},{"path":"https://liu-chao.site/LBDiscover/reference/standard_validation.html","id":null,"dir":"Reference","previous_headings":"","what":"Standard validation method using hypergeometric tests — standard_validation","title":"Standard validation method using hypergeometric tests — standard_validation","text":"Standard validation method using hypergeometric tests","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/standard_validation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standard validation method using hypergeometric tests — standard_validation","text":"","code":"standard_validation(abc_results, co_matrix, alpha, correction)"},{"path":"https://liu-chao.site/LBDiscover/reference/valid_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter entities to include only valid biomedical terms — valid_entities","title":"Filter entities to include only valid biomedical terms — valid_entities","text":"function applies validation ensure legitimate biomedical entities included, preserving trusted terms.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/valid_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter entities to include only valid biomedical terms — valid_entities","text":"","code":"valid_entities(   entities,   primary_term,   primary_term_variations = NULL,   validation_function = NULL,   verbose = TRUE,   entity_col = \"entity\",   type_col = \"entity_type\" )"},{"path":"https://liu-chao.site/LBDiscover/reference/valid_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter entities to include only valid biomedical terms — valid_entities","text":"entities Data frame entities filter primary_term primary term trust primary_term_variations Vector variations primary term trust validation_function Function validate entities (default: is_valid_biomedical_entity) verbose Logical; TRUE, print status messages (default: TRUE) entity_col Name column containing entity names (default: \"entity\") type_col Name column containing entity types (default: \"entity_type\")","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/valid_entities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter entities to include only valid biomedical terms — valid_entities","text":"data frame filtered entities","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_abc.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply statistical validation to ABC model results with support for large matrices — validate_abc","title":"Apply statistical validation to ABC model results with support for large matrices — validate_abc","text":"function performs statistical tests validate ABC model results. calculates p-values using hypergeometric tests applies correction multiple testing. function optimized work large co-occurrence matrices.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_abc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply statistical validation to ABC model results with support for large matrices — validate_abc","text":"","code":"validate_abc(   abc_results,   co_matrix,   alpha = 0.05,   correction = c(\"BH\", \"bonferroni\", \"none\"),   filter_by_significance = FALSE )"},{"path":"https://liu-chao.site/LBDiscover/reference/validate_abc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply statistical validation to ABC model results with support for large matrices — validate_abc","text":"abc_results data frame containing ABC results. co_matrix co-occurrence matrix used generate ABC results. alpha Significance level (p-value threshold). correction Method multiple testing correction. filter_by_significance Logical. TRUE, returns significant results.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_abc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply statistical validation to ABC model results with support for large matrices — validate_abc","text":"data frame ABC results statistical significance measures.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_abc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply statistical validation to ABC model results with support for large matrices — validate_abc","text":"","code":"if (FALSE) { # \\dontrun{ validated_results <- validate_abc(abc_results, co_matrix) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/validate_biomedical_entity.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate biomedical entities using BioBERT or other ML models — validate_biomedical_entity","title":"Validate biomedical entities using BioBERT or other ML models — validate_biomedical_entity","text":"Validate biomedical entities using BioBERT ML models","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_biomedical_entity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate biomedical entities using BioBERT or other ML models — validate_biomedical_entity","text":"","code":"validate_biomedical_entity(term, claimed_type)"},{"path":"https://liu-chao.site/LBDiscover/reference/validate_biomedical_entity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate biomedical entities using BioBERT or other ML models — validate_biomedical_entity","text":"term Character string, term validate claimed_type Character string, claimed entity type","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_biomedical_entity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate biomedical entities using BioBERT or other ML models — validate_biomedical_entity","text":"Logical indicating term validated","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_comprehensive.html","id":null,"dir":"Reference","previous_headings":"","what":"Comprehensive entity validation using multiple techniques — validate_entity_comprehensive","title":"Comprehensive entity validation using multiple techniques — validate_entity_comprehensive","text":"Comprehensive entity validation using multiple techniques","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_comprehensive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Comprehensive entity validation using multiple techniques — validate_entity_comprehensive","text":"","code":"validate_entity_comprehensive(   term,   claimed_type,   use_nlp = TRUE,   use_pattern = TRUE,   use_external_api = FALSE )"},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_comprehensive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Comprehensive entity validation using multiple techniques — validate_entity_comprehensive","text":"term Character string, term validate claimed_type Character string, claimed entity type use_nlp Logical, whether use NLP-based validation use_pattern Logical, whether use pattern-based validation use_external_api Logical, whether query external APIs","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_comprehensive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Comprehensive entity validation using multiple techniques — validate_entity_comprehensive","text":"Logical indicating term validated","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_with_nlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate entity types using NLP-based entity recognition with improved accuracy — validate_entity_with_nlp","title":"Validate entity types using NLP-based entity recognition with improved accuracy — validate_entity_with_nlp","text":"Validate entity types using NLP-based entity recognition improved accuracy","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_with_nlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate entity types using NLP-based entity recognition with improved accuracy — validate_entity_with_nlp","text":"","code":"validate_entity_with_nlp(term, claimed_type, nlp_model = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_with_nlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate entity types using NLP-based entity recognition with improved accuracy — validate_entity_with_nlp","text":"term Character string, term validate claimed_type Character string, claimed entity type nlp_model loaded NLP model use validation","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_entity_with_nlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate entity types using NLP-based entity recognition with improved accuracy — validate_entity_with_nlp","text":"Logical indicating term likely claimed type","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_umls_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate a UMLS API key — validate_umls_key","title":"Validate a UMLS API key — validate_umls_key","text":"function validates UMLS API key using validation endpoint.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_umls_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate a UMLS API key — validate_umls_key","text":"","code":"validate_umls_key(api_key, validator_api_key = NULL)"},{"path":"https://liu-chao.site/LBDiscover/reference/validate_umls_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate a UMLS API key — validate_umls_key","text":"api_key UMLS API key validate validator_api_key application's UMLS API key (third-party validation)","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_umls_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate a UMLS API key — validate_umls_key","text":"Logical indicating API key valid","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/validate_umls_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate a UMLS API key — validate_umls_key","text":"","code":"if (FALSE) { # \\dontrun{ is_valid <- validate_umls_key(\"user_api_key\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/vec_preprocess.html","id":null,"dir":"Reference","previous_headings":"","what":"Vectorized preprocessing of text — vec_preprocess","title":"Vectorized preprocessing of text — vec_preprocess","text":"function preprocesses text data using vectorized operations better performance. function preprocesses text data using vectorized operations better performance.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vec_preprocess.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vectorized preprocessing of text — vec_preprocess","text":"","code":"vec_preprocess(   text_data,   text_column = \"abstract\",   remove_stopwords = TRUE,   custom_stopwords = NULL,   min_word_length = 3,   max_word_length = 50,   chunk_size = 100 )  vec_preprocess(   text_data,   text_column = \"abstract\",   remove_stopwords = TRUE,   custom_stopwords = NULL,   min_word_length = 3,   max_word_length = 50,   chunk_size = 100 )"},{"path":"https://liu-chao.site/LBDiscover/reference/vec_preprocess.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vectorized preprocessing of text — vec_preprocess","text":"text_data data frame containing text data. text_column Name column containing text process. remove_stopwords Logical. TRUE, removes stopwords. custom_stopwords Character vector additional stopwords remove. min_word_length Minimum word length keep. max_word_length Maximum word length keep. chunk_size Number documents process chunk.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vec_preprocess.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vectorized preprocessing of text — vec_preprocess","text":"data frame processed text. data frame processed text.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vec_preprocess.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vectorized preprocessing of text — vec_preprocess","text":"","code":"if (FALSE) { # \\dontrun{ processed_data <- vec_preprocess(article_data, text_column = \"abstract\") } # } if (FALSE) { # \\dontrun{ processed_data <- vec_preprocess(article_data, text_column = \"abstract\") } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/vis_abc_heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a heatmap of ABC connections — vis_abc_heatmap","title":"Create a heatmap of ABC connections — vis_abc_heatmap","text":"function creates heatmap visualization ABC connections using base R graphics.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_abc_heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a heatmap of ABC connections — vis_abc_heatmap","text":"","code":"vis_abc_heatmap(   abc_results,   top_n = 25,   min_score = 0.1,   show_labels = TRUE,   title = \"ABC Connections Heatmap\" )"},{"path":"https://liu-chao.site/LBDiscover/reference/vis_abc_heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a heatmap of ABC connections — vis_abc_heatmap","text":"abc_results data frame containing ABC results apply_abc_model(). top_n Number top results visualize. min_score Minimum score threshold including connections. show_labels Logical. TRUE, shows labels tiles. title Plot title.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_abc_heatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a heatmap of ABC connections — vis_abc_heatmap","text":"NULL invisibly. function creates plot side effect.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_abc_heatmap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a heatmap of ABC connections — vis_abc_heatmap","text":"","code":"if (FALSE) { # \\dontrun{ vis_abc_heatmap(abc_results, top_n = 20) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/vis_heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an enhanced heatmap of ABC connections — vis_heatmap","title":"Create an enhanced heatmap of ABC connections — vis_heatmap","text":"function creates improved heatmap visualization ABC connections can display entity type information available, without enforcing type constraints.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an enhanced heatmap of ABC connections — vis_heatmap","text":"","code":"vis_heatmap(   abc_results,   top_n = 25,   min_score = 0.1,   show_significance = TRUE,   color_palette = \"blues\",   title = \"ABC Connections Heatmap\",   show_entity_types = TRUE )"},{"path":"https://liu-chao.site/LBDiscover/reference/vis_heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an enhanced heatmap of ABC connections — vis_heatmap","text":"abc_results data frame containing ABC results. top_n Number top results visualize. min_score Minimum score threshold including connections. show_significance Logical. TRUE, marks significant connections. color_palette Character. Color palette use heatmap. title Plot title. show_entity_types Logical. TRUE, includes entity types axis labels.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_heatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an enhanced heatmap of ABC connections — vis_heatmap","text":"NULL invisibly. function creates plot side effect.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_heatmap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an enhanced heatmap of ABC connections — vis_heatmap","text":"","code":"if (FALSE) { # \\dontrun{ vis_heatmap(abc_results, top_n = 20, show_significance = TRUE) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/vis_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an enhanced network visualization of ABC connections — vis_network","title":"Create an enhanced network visualization of ABC connections — vis_network","text":"function creates improved network visualization ABC connections displays entity types available, without enforcing type constraints.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an enhanced network visualization of ABC connections — vis_network","text":"","code":"vis_network(   abc_results,   top_n = 25,   min_score = 0.1,   show_significance = TRUE,   node_size_factor = 5,   color_by = \"type\",   title = \"ABC Model Network\",   show_entity_types = TRUE,   label_size = 1 )"},{"path":"https://liu-chao.site/LBDiscover/reference/vis_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an enhanced network visualization of ABC connections — vis_network","text":"abc_results data frame containing ABC results. top_n Number top results visualize. min_score Minimum score threshold including connections. show_significance Logical. TRUE, highlights significant connections. node_size_factor Factor scaling node sizes. color_by Column use node colors. Default 'type'. title Plot title. show_entity_types Logical. TRUE, includes entity types node labels. label_size Relative size labels. Default 1.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an enhanced network visualization of ABC connections — vis_network","text":"NULL invisibly. function creates plot side effect.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/vis_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an enhanced network visualization of ABC connections — vis_network","text":"","code":"if (FALSE) { # \\dontrun{ vis_network(abc_results, top_n = 20, show_significance = TRUE,                      show_entity_types = TRUE) } # }"},{"path":"https://liu-chao.site/LBDiscover/reference/visualize_abc_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize ABC model results as a network — visualize_abc_network","title":"Visualize ABC model results as a network — visualize_abc_network","text":"Create network visualization ABC connections using base R graphics.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/visualize_abc_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize ABC model results as a network — visualize_abc_network","text":"","code":"vis_abc_network(   abc_results,   top_n = 25,   min_score = 0.1,   node_size_factor = 3,   edge_width_factor = 1,   color_by = \"type\",   title = \"ABC Model Network\" )"},{"path":"https://liu-chao.site/LBDiscover/reference/visualize_abc_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize ABC model results as a network — visualize_abc_network","text":"abc_results data frame containing ABC results apply_abc_model(). top_n Number top results visualize. min_score Minimum score threshold including connections. node_size_factor Factor scaling node sizes. edge_width_factor Factor scaling edge widths. color_by Column use node colors. Default 'type'. title Plot title.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/visualize_abc_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize ABC model results as a network — visualize_abc_network","text":"NULL invisibly. function creates plot side effect.","code":""},{"path":"https://liu-chao.site/LBDiscover/reference/visualize_abc_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize ABC model results as a network — visualize_abc_network","text":"","code":"if (FALSE) { # \\dontrun{ # Create a network visualization of ABC model results vis_abc_network(abc_results, top_n = 20) } # }"},{"path":"https://liu-chao.site/LBDiscover/news/index.html","id":"lbdiscover-010","dir":"Changelog","previous_headings":"","what":"LBDiscover 0.1.0","title":"LBDiscover 0.1.0","text":"Initial CRAN submission.","code":""}]
